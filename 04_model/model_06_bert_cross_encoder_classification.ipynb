{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 06 Bert Cross Entropy Classification for Label Prediction\n",
    "\n",
    "Prediction of claim labels based on the matched evidence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to project root\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT_DIR = Path.cwd()\n",
    "while not ROOT_DIR.joinpath(\"src\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device is 'mps'\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score\n",
    "\n",
    "from src.logger import SimpleLogger\n",
    "from src.model_05 import BertCrossEncoderClassifier\n",
    "from src.data import LabelClassificationDataset\n",
    "from src.torch_utils import get_torch_device\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import exp\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "TORCH_DEVICE = get_torch_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ROOT_DIR.joinpath(\"./result/models/*\")\n",
    "DATA_PATH = ROOT_DIR.joinpath(\"./data/*\")\n",
    "LOG_PATH = ROOT_DIR.joinpath(\"./result/logs/*\")\n",
    "SHORTLIST_PATH = ROOT_DIR.joinpath(\"./result/pipeline/shortlisting_v2/*\")\n",
    "\n",
    "run_time = datetime.now().strftime('%Y_%m_%d_%H_%M')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    model,\n",
    "    claims_paths:List[Path],\n",
    "    save_path:Path=None,\n",
    "    label_weight:list=None,\n",
    "    label_smoothing:float=None,\n",
    "    warmup:float=0.1,\n",
    "    lr:float=0.00005, # 5e-5\n",
    "    weight_decay:float=0.01,\n",
    "    normalize_text:bool=True,\n",
    "    max_length:int=128,\n",
    "    dropout:float=None,\n",
    "    n_epochs:int=5,\n",
    "    batch_size:int=64,\n",
    "):\n",
    "    # Generate training dataset\n",
    "    train_data = LabelClassificationDataset(\n",
    "        claims_paths=claims_paths,\n",
    "        training=True,\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Generate evaluation dataset\n",
    "    dev_data = LabelClassificationDataset(\n",
    "        claims_paths=[Path(\"./data/dev-claims.json\")],\n",
    "        training=True,\n",
    "    )\n",
    "    dev_dataloader = DataLoader(\n",
    "        dataset=dev_data,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = CrossEntropyLoss(\n",
    "        weight=torch.tensor(label_weight, device=TORCH_DEVICE),\n",
    "        label_smoothing=label_smoothing\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(\n",
    "        params=model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler = LinearLR(\n",
    "        optimizer=optimizer,\n",
    "        total_iters=warmup * len(train_dataloader),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy_fn = MulticlassAccuracy()\n",
    "    f1_fn = MulticlassF1Score()\n",
    "    \n",
    "    # Training epochs --------------------------------------------------------\n",
    "    \n",
    "    best_epoch_loss = 999\n",
    "    best_epoch_f1 = -1\n",
    "    best_epoch_acc = -1\n",
    "    best_epoch = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print(f\"\\nEpoch: {epoch + 1} of {n_epochs}\\n\")\n",
    "        \n",
    "        # Run training -------------------------------------------------------\n",
    "        model.train()\n",
    "        \n",
    "        train_batches = tqdm(train_dataloader, desc=\"train batches\")\n",
    "        running_losses = []\n",
    "        for batch in train_batches:\n",
    "            claim_texts, evidence_texts, labels, claim_ids, evidence_ids = batch\n",
    "            texts = list(zip(claim_texts, evidence_texts))\n",
    "            \n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + loss\n",
    "            output, logits, seq = model(\n",
    "                texts=texts,\n",
    "                normalize_text=normalize_text,\n",
    "                max_length=max_length,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            # Backward + optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            batch_loss = loss.item() * len(batch)\n",
    "            running_losses.append(batch_loss)\n",
    "            \n",
    "            train_batches.postfix = f\"loss: {batch_loss:.3f}\"\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # Epoch loss\n",
    "        epoch_loss = np.average(running_losses)\n",
    "        print(f\"Average epoch loss: {epoch_loss:.3f}\")\n",
    "    \n",
    "        # Run evaluation ------------------------------------------------------\n",
    "        model.eval()\n",
    "\n",
    "        dev_batches = tqdm(dev_dataloader, desc=\"dev batches\")\n",
    "        dev_acc = []\n",
    "        dev_f1 = []\n",
    "        for batch in dev_batches:\n",
    "            claim_texts, evidence_texts, labels, claim_ids, evidence_ids = batch\n",
    "            texts = list(zip(claim_texts, evidence_texts))\n",
    "\n",
    "            # Forward\n",
    "            output, logits, seq = model(\n",
    "                texts=texts,\n",
    "                normalize_text=normalize_text,\n",
    "                max_length=max_length,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            \n",
    "            # Prediction\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "\n",
    "            # Metrics\n",
    "            accuracy_fn.update(predicted.cpu(), labels.cpu())\n",
    "            f1_fn.update(predicted.cpu(), labels.cpu())\n",
    "            \n",
    "            acc = accuracy_fn.compute()\n",
    "            f1 = f1_fn.compute()\n",
    "            \n",
    "            dev_acc.append(acc)\n",
    "            dev_f1.append(f1)\n",
    "            \n",
    "            dev_batches.postfix = f\" acc: {acc:.3f}, f1: {f1:.3f}\"\n",
    "\n",
    "            continue\n",
    "        \n",
    "        # Consider metrics\n",
    "        epoch_acc = np.average(dev_acc)\n",
    "        print(f\"Average epoch accuracy: {epoch_acc:.3f}\")\n",
    "        \n",
    "        epoch_f1 = np.average(dev_f1)\n",
    "        print(f\"Average epoch f1: {epoch_f1:.3f}\")\n",
    "        \n",
    "        if epoch_acc > best_epoch_acc:\n",
    "            best_epoch_acc = epoch_acc\n",
    "        \n",
    "        if epoch_f1 > best_epoch_f1:\n",
    "            best_epoch_f1 = epoch_f1\n",
    "            best_epoch = epoch + 1\n",
    "        \n",
    "        # Save model ----------------------------------------------------------\n",
    "        \n",
    "        # Save the model with the best f1 score\n",
    "        if save_path and epoch_f1 >= best_epoch_f1:\n",
    "            torch.save(model, save_path)\n",
    "            print(f\"Saved model to: {save_path}\")\n",
    "        \n",
    "    print(\"Done!\")\n",
    "    return best_epoch_acc, best_epoch_f1, best_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a blank pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertCrossEncoderClassifier(\n",
    "    pretrained_name=\"bert-base-uncased\",\n",
    "    n_classes=3,\n",
    "    device=TORCH_DEVICE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load one previously trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_SAVE_PATH = MODEL_PATH.with_name(\"\")\n",
    "# with open(MODEL_PATH.with_name(MODEL_SAVE_PATH), mode=\"rb\") as f:\n",
    "#     model = torch.load(f, map_location=TORCH_DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:00<00:00, 407968.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=3730\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 581388.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=433\n",
      "\n",
      "Epoch: 1 of 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 156/156 [03:23<00:00,  1.31s/it, loss: 5.070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 5.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 19/19 [00:07<00:00,  2.61it/s,  acc: 0.513, f1: 0.513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch accuracy: 0.539\n",
      "Average epoch f1: 0.539\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_06_bert_base_uncased_cross_encoder_label_2023_05_09_20_05_high_weights.pth\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.53880805, 0.53880805, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    model=model,\n",
    "    claims_paths=[\n",
    "        DATA_PATH.with_name(\"train-claims.json\")\n",
    "    ],\n",
    "    save_path=MODEL_PATH.with_name(f\"model_06_bert_base_uncased_cross_encoder_label_{run_time}_high_weights.pth\"),\n",
    "    warmup=0.1,\n",
    "    lr=0.000005, # 5e-6\n",
    "    weight_decay=0.02,\n",
    "    normalize_text=True,\n",
    "    max_length=512,\n",
    "    dropout=0.1,\n",
    "    n_epochs=1,\n",
    "    label_weight=[2, 1.2, 1],\n",
    "    # label_weight=[1, 0.6, 0.4],\n",
    "    label_smoothing=0.0,\n",
    "    batch_size=24,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = ParameterGrid(param_grid={\n",
    "#     \"claims_paths\": [[\n",
    "#         DATA_PATH.with_name(\"train-claims.json\")\n",
    "#     ]],\n",
    "#     \"warmup\": [0.1],\n",
    "#     \"lr\": [0.000005],\n",
    "#     \"weight_decay\": [0.02],\n",
    "#     \"normalize_text\": [True],\n",
    "#     \"max_length\": [512],\n",
    "#     \"dropout\": [0.1],\n",
    "#     \"n_epochs\": [10],\n",
    "#     \"batch_size\": [24],\n",
    "#     \"freeze_bert\": [False],\n",
    "#     \"label_weight\":[\n",
    "#         # [2, 1.2, 1],\n",
    "#         [1, 0.6, 0.4],\n",
    "#     ],\n",
    "#     \"label_smoothing\": [0.0]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SimpleLogger(\"model_06_cross_encoder_retrieval\") as logger:\n",
    "    logger.set_stream_handler()\n",
    "    logger.set_file_handler(\n",
    "        log_path=LOG_PATH,\n",
    "        filename=\"model_06_hyperparam_tuning.txt\"\n",
    "    )\n",
    "    best_f1 = -1\n",
    "    best_params = {}\n",
    "    for hyperparam in hyperparams:\n",
    "        model = BertCrossEncoderClassifier(\n",
    "            pretrained_name=\"bert-base-uncased\",\n",
    "            n_classes=3,\n",
    "            device=TORCH_DEVICE\n",
    "        )\n",
    "        \n",
    "        model_param = hyperparam.copy()\n",
    "        \n",
    "        # Freeze bert parameters if desired\n",
    "        if \"freeze_bert\" in model_param.keys():\n",
    "            if hyperparam[\"freeze_bert\"] is True:\n",
    "                for param in model.bert.parameters():\n",
    "                    param.requires_grad = False\n",
    "            del model_param[\"freeze_bert\"]\n",
    "        \n",
    "        logger.info(\"\\n== RUN\")\n",
    "        logger.info(hyperparam)\n",
    "        \n",
    "        accuracy, f1, epoch = training_loop(model=model, **model_param)\n",
    "        \n",
    "        logger.info(f\"run_best_epoch: {epoch}, run_best_acc: {accuracy}, run_best_f1: {f1}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_params = hyperparam\n",
    "        \n",
    "        logger.info(f\"\\n== CURRENT BEST F1: {best_f1}\")\n",
    "        logger.info(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp90042_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
