{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 03a\n",
    "\n",
    "Evidence retrieval using a Siamese BERT classification model.\n",
    "This is similar to Model 01, however, it only uses official pre-trained models from hugging face.\n",
    "\n",
    "Ref:\n",
    "- [Hugging face pre-trained models](https://huggingface.co/transformers/v3.3.1/pretrained_models.html)\n",
    "- [Hugging face guide to fine-tuning](https://huggingface.co/transformers/v3.3.1/custom_datasets.html)\n",
    "- [Hugging face guide to fine-tuning easy](https://huggingface.co/docs/transformers/training)\n",
    "- [SO Guide](https://stackoverflow.com/a/64156912)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to project root\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT_DIR = Path.cwd()\n",
    "while not ROOT_DIR.joinpath(\"src\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ROOT_DIR.joinpath(\"./result/models/*\")\n",
    "DATA_PATH = ROOT_DIR.joinpath(\"./data/*\")\n",
    "NER_PATH = ROOT_DIR.joinpath(\"./result/ner/*\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device is 'mps'\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, CosineEmbeddingLoss\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score\n",
    "\n",
    "from src.torch_utils import get_torch_device\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import exp\n",
    "\n",
    "TORCH_DEVICE = get_torch_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClaimEvidencePair:\n",
    "    claim_id:str\n",
    "    evidence_id:str\n",
    "    label:int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseEvalDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dev_claims_path:Path,\n",
    "        evidence_path:Path,\n",
    "        device = None,\n",
    "        verbose:bool=True\n",
    "    ) -> None:\n",
    "        super(SiameseEvalDataset, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        \n",
    "        # Load claims data from json\n",
    "        with open(dev_claims_path, mode=\"r\") as f:\n",
    "            self.claims = (json.load(fp=f))\n",
    "\n",
    "        # Load evidence library\n",
    "        self.evidence = dict()\n",
    "        with open(evidence_path, mode=\"r\") as f:\n",
    "            self.evidence.update(json.load(fp=f))\n",
    "        \n",
    "        # Get a list of all evidences within the dev set\n",
    "        self.related_evidences = sorted({\n",
    "            evidence_id\n",
    "            for claim in self.claims.values()\n",
    "            for evidence_id in claim[\"evidences\"]\n",
    "        })\n",
    "        \n",
    "        # Generate the data\n",
    "        self.data = self.__generate_data()\n",
    "        return\n",
    "        \n",
    "    def __generate_data(self):\n",
    "        data = []\n",
    "        for claim_id, claim in tqdm(\n",
    "            iterable=self.claims.items(),\n",
    "            desc=\"claims\",\n",
    "            disable=not self.verbose\n",
    "        ):\n",
    "            evidence_ids = claim[\"evidences\"]\n",
    "            \n",
    "            # Get the positives\n",
    "            for evidence_id in evidence_ids:\n",
    "                data.append(ClaimEvidencePair(\n",
    "                    claim_id=claim_id,\n",
    "                    evidence_id=evidence_id,\n",
    "                    label=1\n",
    "                ))\n",
    "            \n",
    "            # Get some negatives\n",
    "            n_neg = 0\n",
    "            for rel_evidence_id in self.related_evidences:\n",
    "                if n_neg >= 10:\n",
    "                    break\n",
    "                if rel_evidence_id in evidence_ids:\n",
    "                    continue\n",
    "                data.append(ClaimEvidencePair(\n",
    "                    claim_id=claim_id,\n",
    "                    evidence_id=rel_evidence_id,\n",
    "                    label=-1\n",
    "                ))\n",
    "                n_neg += 1\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[Union[str, torch.Tensor]]:\n",
    "        # Fetch the required data rows\n",
    "        data = self.data[idx]\n",
    "        \n",
    "        # Get the label\n",
    "        label = torch.tensor(data.label, device=self.device)\n",
    "        \n",
    "        # Get text ids\n",
    "        claim_id = data.claim_id\n",
    "        evidence_id = data.evidence_id\n",
    "        \n",
    "        # Get text\n",
    "        claim_text = self.claims[claim_id][\"claim_text\"]\n",
    "        evidence_text = self.evidence[evidence_id]\n",
    "\n",
    "        return (claim_text, evidence_text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        claims_paths:List[Path],\n",
    "        claims_shortlist_paths:List[Path],\n",
    "        evidence_path:Path,\n",
    "        evidence_shortlists:List[Path] = None,\n",
    "        device = None,\n",
    "        n_neg_shortlist:int = 10,\n",
    "        n_neg_general:int = 10,\n",
    "        verbose:bool=True\n",
    "    ) -> None:\n",
    "        super(SiameseDataset, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.n_neg_shortlist = n_neg_shortlist\n",
    "        self.n_neg_general = n_neg_general\n",
    "\n",
    "        # Load claims data from json, this is a list as we could use\n",
    "        # multiple json files in the same dataset\n",
    "        self.claims = dict()\n",
    "        for json_file in claims_paths:\n",
    "            with open(json_file, mode=\"r\") as f:\n",
    "                self.claims.update(json.load(fp=f))\n",
    "                # print(f\"loaded claims: {json_file}\")\n",
    "        \n",
    "        # Load the pre-retrieved shortlist of evidences by claim\n",
    "        self.claims_shortlist = dict()\n",
    "        for json_file in claims_shortlist_paths:\n",
    "            with open(json_file, mode=\"r\") as f:\n",
    "                self.claims_shortlist.update(json.load(fp=f))\n",
    "                # print(f\"loaded claims_shortlist: {json_file}\")\n",
    "        \n",
    "        # Load evidence library\n",
    "        self.evidence = dict()\n",
    "        with open(evidence_path, mode=\"r\") as f:\n",
    "            self.evidence.update(json.load(fp=f))\n",
    "            # print(f\"loaded evidences: {json_file}\")\n",
    "        \n",
    "        # Load the evidence shortlists if available\n",
    "        # Reduce the overall evidence list to the shortlist\n",
    "        if evidence_shortlists is not None:\n",
    "            self.evidence_shortlist = set()\n",
    "            for json_file in evidence_shortlists:\n",
    "                with open(json_file, mode=\"r\") as f:\n",
    "                    self.evidence_shortlist.update(json.load(fp=f))\n",
    "                    # print(f\"loaded evidence shortlist: {json_file}\")\n",
    "        \n",
    "        # print(f\"n_evidences: {len(self.evidence)}\")\n",
    "        \n",
    "        # Generate the data\n",
    "        self.data = self.__generate_data()\n",
    "        return\n",
    "\n",
    "    def __generate_data(self):\n",
    "        print(\"Generate siamese dataset\")\n",
    "        \n",
    "        data = []\n",
    "        for claim_id, claim in tqdm(\n",
    "            iterable=self.claims.items(),\n",
    "            desc=\"claims\",\n",
    "            disable=not self.verbose\n",
    "        ):\n",
    "            # Check if we have evidences supplied, this will inform\n",
    "            # whether this is for training\n",
    "            is_training = \"evidences\" in claim.keys()\n",
    "            pos_evidence_ids = set()\n",
    "            \n",
    "            # Get positive samples from evidences with label=1\n",
    "            if is_training:\n",
    "                pos_evidence_ids.update(claim[\"evidences\"])\n",
    "\n",
    "                for evidence_id in pos_evidence_ids:\n",
    "                    data.append(ClaimEvidencePair(\n",
    "                        claim_id=claim_id,\n",
    "                        evidence_id=evidence_id,\n",
    "                        label=1\n",
    "                    ))\n",
    "                    \n",
    "            # Get negative samples from pre-retrieved evidences\n",
    "            # for each claim with label=-1\n",
    "            retrieved_evidence_ids = self.claims_shortlist.get(claim_id, [])\n",
    "            if len(retrieved_evidence_ids) > 0:\n",
    "                retrieved_neg_evidence_ids = random.sample(\n",
    "                    population=retrieved_evidence_ids,\n",
    "                    k=min(self.n_neg_shortlist, len(retrieved_evidence_ids))\n",
    "                )\n",
    "                \n",
    "                # Generate claim and shortlisted negative evidence pairs\n",
    "                for evidence_id in retrieved_neg_evidence_ids:\n",
    "                    data.append(ClaimEvidencePair(\n",
    "                        claim_id=claim_id,\n",
    "                        evidence_id=evidence_id,\n",
    "                        label=-1\n",
    "                    ))\n",
    "            \n",
    "            # Get negative samples from shortlisted evidences list with label=0\n",
    "            if len(self.evidence_shortlist) > 0:\n",
    "                shortlist_neg_evidence_ids = random.sample(\n",
    "                    population=self.evidence_shortlist,\n",
    "                    k=min(self.n_neg_general, len(self.evidence_shortlist))\n",
    "                )\n",
    "                \n",
    "                # Generate claim and shortlisted negative evidence pairs\n",
    "                for evidence_id in shortlist_neg_evidence_ids:\n",
    "                    data.append(ClaimEvidencePair(\n",
    "                        claim_id=claim_id,\n",
    "                        evidence_id=evidence_id,\n",
    "                        label=-1\n",
    "                    ))\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        print(f\"Generated data n={len(data)}\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[Union[str, torch.Tensor]]:\n",
    "        # Fetch the required data rows\n",
    "        data = self.data[idx]\n",
    "        \n",
    "        # Get the label\n",
    "        label = torch.tensor(data.label, device=self.device)\n",
    "        \n",
    "        # Get text ids\n",
    "        claim_id = data.claim_id\n",
    "        evidence_id = data.evidence_id\n",
    "        \n",
    "        # Get text\n",
    "        claim_text = self.claims[claim_id][\"claim_text\"]\n",
    "        evidence_text = self.evidence[evidence_id]\n",
    "\n",
    "        return (claim_text, evidence_text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL GENERATE THE DATASET PER EPOCH SO TO RANDOMISE THE NEGATIVE SAMPLES\n",
    "\n",
    "# train_data = SiameseDataset(\n",
    "#     claims_paths=[DATA_PATH.with_name(\"train-claims.json\")],\n",
    "#     claims_shortlist_paths=[NER_PATH.with_name(\"train_claim_evidence_retrieved.json\")],\n",
    "#     evidence_shortlists=[NER_PATH.with_name(\"shortlist_train_claim_evidence_retrieved.json\")],\n",
    "#     evidence_path=DATA_PATH.with_name(\"evidence.json\"),\n",
    "#     device=TORCH_DEVICE,\n",
    "#     n_neg_shortlist=100,\n",
    "#     n_neg_general=100\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseEmbedderBert(Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_name:str,\n",
    "            device,\n",
    "            **kwargs\n",
    "        ) -> None:\n",
    "        super(SiameseEmbedderBert, self).__init__(**kwargs)\n",
    "        self.device = device\n",
    "        \n",
    "        # Use a pretrained tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_name)\n",
    "        \n",
    "        # Use a pretrained model\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name)\n",
    "        self.bert.to(device=device)\n",
    "        return\n",
    "        \n",
    "    def forward(self, claim_texts, evidence_texts, eval_mode:bool=False) -> Tuple[torch.Tensor]:\n",
    "        \n",
    "        # Run the tokenizer\n",
    "        t_kwargs = {\n",
    "            \"return_tensors\": \"pt\",\n",
    "            \"padding\": True,\n",
    "            \"truncation\": True,\n",
    "            \"max_length\": 100,\n",
    "            \"add_special_tokens\":True\n",
    "        }\n",
    "        claim_x = self.tokenizer(claim_texts, **t_kwargs)\n",
    "        evidence_x = self.tokenizer(evidence_texts, **t_kwargs)\n",
    "        \n",
    "        claim_x = claim_x[\"input_ids\"].to(device=self.device)\n",
    "        evidence_x = evidence_x[\"input_ids\"].to(device=self.device)\n",
    "        \n",
    "        # Run Bert\n",
    "        claim_x = self.bert(claim_x, return_dict=True).pooler_output\n",
    "        evidence_x = self.bert(evidence_x, return_dict=True).pooler_output\n",
    "        # dim=768\n",
    "        \n",
    "        # Cosine similarity\n",
    "        if eval_mode:\n",
    "            cos_sim = torch.cosine_similarity(x1=claim_x, x2=evidence_x)\n",
    "            return claim_x, evidence_x, cos_sim\n",
    "        \n",
    "        return claim_x, evidence_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SiameseEmbedderBert(\n",
    "    pretrained_name=\"bert-base-cased\",\n",
    "    device=TORCH_DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CosineEmbeddingLoss()\n",
    "optimizer = Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=0.00000128\n",
    ") #! Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "MODEL_NAME = f\"model_03a_bert_base_cos_sim_{run_time}.pth\"\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 199297.38it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_data = SiameseEvalDataset(\n",
    "    dev_claims_path=DATA_PATH.with_name(\"dev-claims.json\"),\n",
    "    evidence_path=DATA_PATH.with_name(\"evidence.json\"),\n",
    "    device=TORCH_DEVICE\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dataset=dev_data,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:08<00:00,  3.63it/s, pos cos_sim: 0.972 neg cos_sim: 0.868]\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation before training to establish baseline\n",
    "model.eval()\n",
    "\n",
    "dev_batches = tqdm(dev_dataloader, desc=\"dev batches\")\n",
    "epoch_pos_cos_sim = []\n",
    "epoch_neg_cos_sim = []\n",
    "for batch in dev_batches:\n",
    "    claim_texts, evidence_texts, labels = batch\n",
    "    \n",
    "    # Forward\n",
    "    claim_emb, evidence_emb, cos_sim = model(claim_texts, evidence_texts, eval_mode=True)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    labelled_cos_sim = cos_sim * labels\n",
    "    pos_cos_sim = labelled_cos_sim[torch.where(labelled_cos_sim > 0)]\n",
    "    neg_cos_sim = labelled_cos_sim[torch.where(labelled_cos_sim < 0)]\n",
    "    \n",
    "    batch_pos_cos_sim = torch.mean(pos_cos_sim).cpu().item()\n",
    "    batch_neg_cos_sim = torch.mean(neg_cos_sim).cpu().item() * -1\n",
    "    \n",
    "    epoch_pos_cos_sim.append(batch_pos_cos_sim)\n",
    "    epoch_neg_cos_sim.append(batch_neg_cos_sim)\n",
    "    \n",
    "    dev_batches.postfix = f\"pos cos_sim: {batch_pos_cos_sim:.3f}\" + \\\n",
    "        f\" neg cos_sim: {batch_neg_cos_sim:.3f}\"\n",
    "    \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.863419, 0.838396\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average cos sim (pos, neg): {np.mean(epoch_pos_cos_sim):3f}, {np.mean(epoch_neg_cos_sim):3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Epoch: 0 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:07<00:00, 175.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:46<00:00,  1.14it/s, loss: 1.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 1.3890747568331474\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.36it/s, pos cos_sim: 0.894 neg cos_sim: 0.791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.828207, 0.789408\n",
      "Epoch: 1 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:07<00:00, 168.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:45<00:00,  1.15it/s, loss: 0.994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 1.0412948363699204\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.32it/s, pos cos_sim: 0.854 neg cos_sim: 0.846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.786210, 0.797201\n",
      "Epoch: 2 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:06<00:00, 175.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:44<00:00,  1.16it/s, loss: 0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 0.6969264437837049\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.35it/s, pos cos_sim: 0.730 neg cos_sim: 0.880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.786910, 0.803039\n",
      "Epoch: 3 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:06<00:00, 175.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:44<00:00,  1.16it/s, loss: 0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 0.6255223235069227\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.36it/s, pos cos_sim: 0.718 neg cos_sim: 0.872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.799902, 0.823209\n",
      "Epoch: 4 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:06<00:00, 178.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:43<00:00,  1.17it/s, loss: 0.560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 0.6126381267200817\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.36it/s, pos cos_sim: 0.655 neg cos_sim: 0.872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.798563, 0.819310\n",
      "Epoch: 5 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:06<00:00, 178.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:43<00:00,  1.17it/s, loss: 1.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 0.5860244390393091\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.42it/s, pos cos_sim: 0.635 neg cos_sim: 0.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.795910, 0.793795\n",
      "Epoch: 6 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:06<00:00, 186.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:42<00:00,  1.17it/s, loss: 0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 0.5642308885893546\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.44it/s, pos cos_sim: 0.624 neg cos_sim: 0.877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.790249, 0.790002\n",
      "Epoch: 7 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:06<00:00, 183.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:43<00:00,  1.17it/s, loss: 0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 0.5422538236891927\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.44it/s, pos cos_sim: 0.662 neg cos_sim: 0.861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.739562, 0.798187\n",
      "Epoch: 8 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:06<00:00, 186.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 121/121 [01:42<00:00,  1.18it/s, loss: 0.372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.2800e-06.\n",
      "Average epoch loss: 0.5334123548520499\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_03a_bert_base_cos_sim_2023_05_03_21_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 32/32 [00:07<00:00,  4.33it/s, pos cos_sim: 0.687 neg cos_sim: 0.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cos sim (pos, neg): 0.773839, 0.813255\n",
      "Epoch: 9 of 100\n",
      "\n",
      "Generate siamese dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:07<00:00, 174.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data n=7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches:  32%|███▏      | 39/121 [00:34<01:12,  1.13it/s, loss: 0.578]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(input1\u001b[39m=\u001b[39mclaim_emb, input2\u001b[39m=\u001b[39mevidence_emb, target\u001b[39m=\u001b[39mlabels)\n\u001b[1;32m     48\u001b[0m \u001b[39m# Backward + optimiser\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     50\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m \u001b[39m# Update running loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metric_accuracy = BinaryAccuracy()\n",
    "metric_f1 = BinaryF1Score()\n",
    "metric_recall = BinaryF1Score()\n",
    "\n",
    "scheduler = LinearLR(\n",
    "    optimizer=optimizer,\n",
    "    start_factor=1,\n",
    "    end_factor=1,\n",
    "    total_iters=2,\n",
    "    verbose=True\n",
    ")\n",
    "best_epoch_loss = 999\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    print(f\"Epoch: {epoch} of {N_EPOCHS}\\n\")\n",
    "    \n",
    "    # Run training\n",
    "    model.train()\n",
    "    \n",
    "    train_data = SiameseDataset(\n",
    "        claims_paths=[DATA_PATH.with_name(\"train-claims.json\")],\n",
    "        claims_shortlist_paths=[NER_PATH.with_name(\"train_claim_evidence_retrieved.json\")],\n",
    "        evidence_shortlists=[NER_PATH.with_name(\"shortlist_train_claim_evidence_retrieved.json\")],\n",
    "        evidence_path=DATA_PATH.with_name(\"evidence.json\"),\n",
    "        device=TORCH_DEVICE,\n",
    "        n_neg_shortlist=2,\n",
    "        n_neg_general=1\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        shuffle=True,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    train_batches = tqdm(train_dataloader, desc=\"train batches\")\n",
    "    running_losses = []\n",
    "    for batch in train_batches:\n",
    "        claim_texts, evidence_texts, labels = batch\n",
    "        \n",
    "        # Reset optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward + loss\n",
    "        claim_emb, evidence_emb = model(claim_texts, evidence_texts)\n",
    "        loss = loss_fn(input1=claim_emb, input2=evidence_emb, target=labels)\n",
    "        \n",
    "        # Backward + optimiser\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        batch_loss = loss.item() * len(batch)\n",
    "        running_losses.append(batch_loss)\n",
    "        \n",
    "        train_batches.postfix = f\"loss: {batch_loss:.3f}\"\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = np.average(running_losses)\n",
    "    print(f\"Average epoch loss: {epoch_loss}\")\n",
    "    \n",
    "    # Save model\n",
    "    if epoch_loss <= best_epoch_loss:\n",
    "        best_epoch_loss = epoch_loss\n",
    "        torch.save(model, MODEL_PATH.with_name(MODEL_NAME))\n",
    "        print(f\"Saved model to: {MODEL_PATH.with_name(MODEL_NAME)}\")\n",
    "    \n",
    "    # Evaluate every 5 epochs\n",
    "    # if epoch % 5 != 0:\n",
    "    #     continue\n",
    "    \n",
    "    # Run evaluation before training to establish baseline\n",
    "    model.eval()\n",
    "\n",
    "    dev_batches = tqdm(dev_dataloader, desc=\"dev batches\")\n",
    "    epoch_pos_cos_sim = []\n",
    "    epoch_neg_cos_sim = []\n",
    "    for batch in dev_batches:\n",
    "        claim_texts, evidence_texts, labels = batch\n",
    "\n",
    "        # Forward\n",
    "        claim_emb, evidence_emb, cos_sim = model(claim_texts, evidence_texts, eval_mode=True)\n",
    "\n",
    "        # Cosine similarity\n",
    "        labelled_cos_sim = cos_sim * labels\n",
    "        pos_cos_sim = labelled_cos_sim[torch.where(labelled_cos_sim > 0)]\n",
    "        neg_cos_sim = labelled_cos_sim[torch.where(labelled_cos_sim < 0)]\n",
    "\n",
    "        batch_pos_cos_sim = torch.mean(pos_cos_sim).cpu().item()\n",
    "        batch_neg_cos_sim = torch.mean(neg_cos_sim).cpu().item() * -1\n",
    "\n",
    "        epoch_pos_cos_sim.append(batch_pos_cos_sim)\n",
    "        epoch_neg_cos_sim.append(batch_neg_cos_sim)\n",
    "\n",
    "        dev_batches.postfix = f\"pos cos_sim: {batch_pos_cos_sim:.3f}\" + \\\n",
    "            f\" neg cos_sim: {batch_neg_cos_sim:.3f}\"\n",
    "\n",
    "        continue\n",
    "    \n",
    "    print(f\"Average cos sim (pos, neg): {np.mean(epoch_pos_cos_sim):3f}, {np.mean(epoch_neg_cos_sim):3f}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp90042_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
