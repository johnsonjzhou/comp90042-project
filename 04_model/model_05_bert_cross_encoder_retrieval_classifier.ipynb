{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model 05 Bert Cross Entropy Classification for Retrieval\n",
    "\n",
    "**This is the final implementation for the retrieval stage**\n",
    "\n",
    "Prerequisites:\n",
    "- Please ensure that shortlists have been created for both `train` and `dev` sets before proceeding using the [Model 02c workbook](./model_02c_fast_shortlisting.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to project root\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT_DIR = Path.cwd()\n",
    "while not ROOT_DIR.joinpath(\"src\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and dependencies\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score\n",
    "\n",
    "from src.torch_utils import get_torch_device\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from src.model_05 import BertCrossEncoderClassifier\n",
    "from src.data import RetrievalWithShortlistDataset, RetrievalDevEvalDataset\n",
    "from src.logger import SimpleLogger\n",
    "\n",
    "TORCH_DEVICE = get_torch_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ROOT_DIR.joinpath(\"./result/*\")\n",
    "DATA_PATH = ROOT_DIR.joinpath(\"./data/*\")\n",
    "LOG_PATH = ROOT_DIR.joinpath(\"./result/*\")\n",
    "SHORTLIST_PATH = ROOT_DIR.joinpath(\"./result/*\")\n",
    "\n",
    "run_time = datetime.now().strftime('%Y_%m_%d_%H_%M')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    model,\n",
    "    claims_paths:List[Path],\n",
    "    claims_shortlist_paths:List[Path],\n",
    "    save_path:Path=None,\n",
    "    n_neg_samples:int=5,\n",
    "    warmup:float=0.1,\n",
    "    lr:float=0.00005, # 5e-5\n",
    "    weight_decay:float=0.01,\n",
    "    normalize_text:bool=True,\n",
    "    max_length:int=128,\n",
    "    dropout:float=None,\n",
    "    n_epochs:int=5,\n",
    "    batch_size:int=64,\n",
    "):\n",
    "    # Generate training dataset\n",
    "    train_data = RetrievalWithShortlistDataset(\n",
    "        claims_paths=claims_paths,\n",
    "        claims_shortlist_paths=claims_shortlist_paths,\n",
    "        n_neg_samples=n_neg_samples,\n",
    "        pos_label=1,\n",
    "        neg_label=0\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Generate evaluation dataset\n",
    "    dev_data = RetrievalDevEvalDataset(\n",
    "        n_neg_samples=3,\n",
    "        # n_neg_samples=n_neg_samples,\n",
    "        pos_label=1,\n",
    "        neg_label=0,\n",
    "    )\n",
    "    dev_dataloader = DataLoader(\n",
    "        dataset=dev_data,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(\n",
    "        params=model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler = LinearLR(\n",
    "        optimizer=optimizer,\n",
    "        total_iters=warmup * len(train_dataloader),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy_fn = BinaryAccuracy()\n",
    "    f1_fn = BinaryF1Score()\n",
    "    \n",
    "    # Training epochs --------------------------------------------------------\n",
    "    \n",
    "    best_epoch_loss = 999\n",
    "    best_epoch_f1 = -1\n",
    "    best_epoch_acc = -1\n",
    "    best_epoch = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print(f\"Epoch: {epoch + 1} of {n_epochs}\\n\")\n",
    "        \n",
    "        # Run training -------------------------------------------------------\n",
    "        model.train()\n",
    "        \n",
    "        train_batches = tqdm(train_dataloader, desc=\"train batches\")\n",
    "        running_losses = []\n",
    "        for batch in train_batches:\n",
    "            claim_texts, evidence_texts, labels, claim_ids, evidence_ids = batch\n",
    "            texts = list(zip(claim_texts, evidence_texts))\n",
    "            \n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + loss\n",
    "            output, logits, seq = model(\n",
    "                texts=texts,\n",
    "                normalize_text=normalize_text,\n",
    "                max_length=max_length,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            # Backward + optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            batch_loss = loss.item() * len(batch)\n",
    "            running_losses.append(batch_loss)\n",
    "            \n",
    "            train_batches.postfix = f\"loss: {batch_loss:.3f}\"\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # Epoch loss\n",
    "        epoch_loss = np.average(running_losses)\n",
    "        print(f\"Average epoch loss: {epoch_loss:.3f}\")\n",
    "    \n",
    "        # Run evaluation ------------------------------------------------------\n",
    "        model.eval()\n",
    "\n",
    "        dev_batches = tqdm(dev_dataloader, desc=\"dev batches\")\n",
    "        dev_acc = []\n",
    "        dev_f1 = []\n",
    "        for batch in dev_batches:\n",
    "            claim_texts, evidence_texts, labels, claim_ids, evidence_ids = batch\n",
    "            texts = list(zip(claim_texts, evidence_texts))\n",
    "\n",
    "            # Forward\n",
    "            output, logits, seq = model(\n",
    "                texts=texts,\n",
    "                normalize_text=normalize_text,\n",
    "                max_length=max_length,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            \n",
    "            # Prediction\n",
    "            _, predicted = torch.max(output, dim=-1)\n",
    "\n",
    "            # Metrics\n",
    "            accuracy_fn.update(predicted.cpu(), labels.cpu())\n",
    "            f1_fn.update(predicted.cpu(), labels.cpu())\n",
    "            \n",
    "            acc = accuracy_fn.compute()\n",
    "            f1 = f1_fn.compute()\n",
    "            \n",
    "            dev_acc.append(acc)\n",
    "            dev_f1.append(f1)\n",
    "            \n",
    "            dev_batches.postfix = f\" acc: {acc:.3f}, f1: {f1:.3f}\"\n",
    "\n",
    "            continue\n",
    "        \n",
    "        # Consider metrics\n",
    "        epoch_acc = np.average(dev_acc)\n",
    "        print(f\"Average epoch accuracy: {epoch_acc:.3f}\")\n",
    "        \n",
    "        epoch_f1 = np.average(dev_f1)\n",
    "        print(f\"Average epoch f1: {epoch_f1:.3f}\")\n",
    "        \n",
    "        if epoch_acc > best_epoch_acc:\n",
    "            best_epoch_acc = epoch_acc\n",
    "        \n",
    "        if epoch_f1 > best_epoch_f1:\n",
    "            best_epoch_f1 = epoch_f1\n",
    "            best_epoch = epoch + 1\n",
    "        \n",
    "        # Save model ----------------------------------------------------------\n",
    "        \n",
    "        # Save the model with the best f1 score\n",
    "        if save_path and epoch_f1 >= best_epoch_f1:\n",
    "            torch.save(model, save_path)\n",
    "            print(f\"Saved model to: {save_path}\")\n",
    "        \n",
    "    print(\"Done!\")\n",
    "    return best_epoch_acc, best_epoch_f1, best_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a blank pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertCrossEncoderClassifier(\n",
    "    pretrained_name=\"bert-base-uncased\",\n",
    "    n_classes=2,\n",
    "    device=TORCH_DEVICE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load one previously trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_SAVE_PATH = MODEL_PATH.with_name(\"model_05_bert_cross_encoder_retrieval_2023_05_08_17_06.pth\")\n",
    "# with open(MODEL_SAVE_PATH, mode=\"rb\") as f:\n",
    "#     model = torch.load(f, map_location=TORCH_DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(\n",
    "    model=model,\n",
    "    claims_paths=[\n",
    "        DATA_PATH.with_name(\"train-claims.json\"),\n",
    "        DATA_PATH.with_name(\"dev-claims.json\")\n",
    "    ],\n",
    "    claims_shortlist_paths=[\n",
    "        Path(\"./result/train_shortlist_evidences_max_500.json\"),\n",
    "        Path(\"./result/dev_shortlist_evidences_max_500.json\"),\n",
    "    ],\n",
    "    # save_path=MODEL_PATH.with_name(f\"model_05_bert_cross_encoder_retrieval_2023_05_08_17_06_e6.pth\"),\n",
    "    save_path=MODEL_PATH.with_name(f\"model_05_bert_cross_encoder_retrieval_{run_time}.pth\"),\n",
    "    n_neg_samples=100,\n",
    "    warmup=0.1,\n",
    "    lr=0.000005, # 5e-6\n",
    "    weight_decay=0.02,\n",
    "    normalize_text=True,\n",
    "    max_length=512,\n",
    "    dropout=0.1,\n",
    "    n_epochs=1,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = ParameterGrid(param_grid={\n",
    "#     \"claims_paths\": [[\n",
    "#         DATA_PATH.with_name(\"train-claims.json\")\n",
    "#     ]],\n",
    "#     \"claims_shortlist_paths\": [[\n",
    "#         Path(\"./result/pipeline/shortlisting_v2/train_retrieved_evidences_max_500_idf_no_rel.json\"),\n",
    "#     ]],\n",
    "#     \"n_neg_samples\": [3, 5, 10],\n",
    "#     \"warmup\": [0.1],\n",
    "#     \"lr\": [0.00005, 0.0005],\n",
    "#     \"weight_decay\": [0.01, 0.02],\n",
    "#     \"normalize_text\": [True, False],\n",
    "#     \"max_length\": [512],\n",
    "#     \"dropout\": [None, 0.1],\n",
    "#     \"n_epochs\": [5, 10],\n",
    "#     \"batch_size\": [24]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with SimpleLogger(\"model_05_cross_encoder_retrieval\") as logger:\n",
    "#     logger.set_stream_handler()\n",
    "#     logger.set_file_handler(\n",
    "#         log_path=LOG_PATH,\n",
    "#         filename=\"model_05_hyperparam_tuning.txt\"\n",
    "#     )\n",
    "#     best_f1 = -1\n",
    "#     best_params = {}\n",
    "#     for hyperparam in hyperparams:\n",
    "#         model = BertCrossEncoderClassifier(\n",
    "#             pretrained_name=\"bert-base-uncased\",\n",
    "#             n_classes=2,\n",
    "#             device=TORCH_DEVICE\n",
    "#         )\n",
    "#         logger.info(\"== RUN\")\n",
    "#         logger.info(hyperparam)\n",
    "        \n",
    "#         accuracy, f1, epoch = training_loop(model=model, **hyperparam)\n",
    "        \n",
    "#         logger.info(f\"run_best_epoch: {epoch}, run_best_acc: {accuracy}, run_best_f1: {f1}\")\n",
    "        \n",
    "#         if f1 > best_f1:\n",
    "#             best_f1 = f1\n",
    "#             best_params = hyperparam\n",
    "        \n",
    "#         logger.info(f\"== CURRENT BEST F1: {best_f1}\")\n",
    "#         logger.info(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp90042_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
