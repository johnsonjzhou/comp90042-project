{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 02: Fast evidence shortlisting by information extraction\n",
    "\n",
    "Same as 02a, adding in IDF scoring.\n",
    "\n",
    "**This is the final implementation for the Shortlisting Stage**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [Universal dependencies scheme](https://universaldependencies.org/u/pos/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to project root\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT_DIR = Path.cwd()\n",
    "while not ROOT_DIR.joinpath(\"src\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import json\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, DefaultDict\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from math import floor\n",
    "\n",
    "from src.data import load_as_dataframe, slice_by_claim, SetEncoder\n",
    "from src.normalize import normalize_pipeline\n",
    "from src.ner import \\\n",
    "    train_noun_relations, \\\n",
    "    get_evidence_by_noun, \\\n",
    "    retrieve_claim_evidence_by_noun, \\\n",
    "    view_claim_noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nlp(\"there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in test:\n",
    "    print(t.pos_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = [\"train-claims\", \"dev-claims\", \"evidence\"]\n",
    "train_claims, dev_claims, all_evidences \\\n",
    "    = load_as_dataframe(data_names, full_evidence=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = dev_claims.loc[\"claim-1896\"].reset_index()\n",
    "pair_index = 0\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_doc = nlp(normalize_pipeline(pairs.iloc[pair_index][\"claim_text\"]))\n",
    "evidence_doc = nlp(normalize_pipeline(pairs.iloc[pair_index][\"evidence_text\"]))\n",
    "displacy.render(claim_doc, style=\"dep\")\n",
    "displacy.render(evidence_doc, style=\"dep\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info Tag extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InfoTag:\n",
    "    tag:str\n",
    "    verb_pos:int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_tags(doc, go_nouns:List[str] = []) -> List[InfoTag]:\n",
    "    \"\"\"\n",
    "    Gets info tags (keywords) from a spacy doc, optionally taking\n",
    "    a set of go nouns that must be included regardless of its POS tag.\n",
    "    \"\"\"\n",
    "    \n",
    "    info_tags = list()\n",
    "    seen = list()\n",
    "    seen_tags = list()\n",
    "    \n",
    "    def add_tag(tag_txt:str, info_tags=info_tags, seen_tags=seen_tags):\n",
    "        if tag_txt not in seen_tags:\n",
    "            info_tags += [InfoTag(tag = tag_txt, verb_pos=verb_pos)]\n",
    "            seen_tags += [tag_txt]\n",
    "        return\n",
    "    \n",
    "    verb_pos = 0\n",
    "    for token in (doc):\n",
    "        \n",
    "        # Skip if we have seen the token before\n",
    "        if token.lemma_ in seen:\n",
    "            continue\n",
    "        \n",
    "        # Increment the relative verb position\n",
    "        if token.pos_ in [\"VERB\"]:\n",
    "            verb_pos += 1\n",
    "        \n",
    "        # Include it if it is in the whitelist of go_nouns\n",
    "        if token.lemma_ in go_nouns:\n",
    "            # info_tags += [InfoTag(tag=token.lemma_, verb_pos=verb_pos)]\n",
    "            tag_txt = token.lemma_\n",
    "            add_tag(tag_txt)\n",
    "        \n",
    "        # Include it if it is a (proper)noun\n",
    "        if token.pos_ in [\"PROPN\", \"NOUN\"]:\n",
    "            # info_tags += [InfoTag(tag=token.lemma_, verb_pos=verb_pos)]\n",
    "            tag_txt = token.lemma_\n",
    "            add_tag(tag_txt)\n",
    "        \n",
    "        # Merge proper noun compounds\n",
    "        if token.pos_ in [\"PROPN\"]:\n",
    "            tag = [token.lemma_]\n",
    "            seen += [token.lemma_]\n",
    "            this_token = token\n",
    "            while this_token.dep_ in [\"compound\"]:\n",
    "                this_token = this_token.head\n",
    "                tag += [this_token.lemma_]\n",
    "                seen += [this_token.lemma_]\n",
    "            tag_txt = \" \".join(tag)\n",
    "            add_tag(tag_txt)\n",
    "            # continue\n",
    "        \n",
    "        # Merge noun compounds\n",
    "        if (token.pos_ in [\"NOUN\"]\n",
    "            and token.dep_ in [\"compound\"]\n",
    "            and token.head.pos_ == \"NOUN\"\n",
    "        ):\n",
    "            tag = [token.lemma_, token.head.lemma_]\n",
    "            seen += tag\n",
    "            # info_tags += [InfoTag(tag = \" \".join(tag), verb_pos=verb_pos)]\n",
    "            tag_txt = \" \".join(tag)\n",
    "            add_tag(tag_txt)\n",
    "            # continue\n",
    "        \n",
    "        # Merge Nouns with adjective modifiers\n",
    "        if (\n",
    "            token.pos_ in [\"ADJ\"]\n",
    "            and token.dep_ in [\"amod\"]\n",
    "            and token.head.pos_ in [\"NOUN\"]\n",
    "            and token.head.dep_ not in [\"compound\"]\n",
    "        ):\n",
    "            tag = [token.lemma_, token.head.lemma_]\n",
    "            seen += tag\n",
    "            # info_tags += [InfoTag(tag = \" \".join(tag), verb_pos=verb_pos)]\n",
    "            tag_txt = \" \".join(tag)\n",
    "            add_tag(tag_txt)\n",
    "            # continue\n",
    "        \n",
    "        # Adjectives linked to verbs and nouns\n",
    "        if (\n",
    "            token.pos_ in [\"ADJ\"]\n",
    "            and token.head.pos_ in [\"VERB\", \"NOUN\"]\n",
    "        ):\n",
    "            tag = [token.lemma_]\n",
    "            seen += tag\n",
    "            # info_tags += [InfoTag(tag = \" \".join(tag), verb_pos=verb_pos)]\n",
    "            tag_txt = \" \".join(tag)\n",
    "            add_tag(tag_txt)\n",
    "            # continue\n",
    "        \n",
    "        continue\n",
    "\n",
    "    return info_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_tags = get_info_tags(claim_doc)\n",
    "claim_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_tags = get_info_tags(evidence_doc)\n",
    "evidence_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = ROOT_DIR.joinpath(\"./data/*\")\n",
    "SAVE_PATH = ROOT_DIR.joinpath(\"./result/*\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info tag pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_tag_pipeline(\n",
    "    text:str,\n",
    "    return_doc:bool=False\n",
    ") -> List[InfoTag]:\n",
    "    \"\"\"\n",
    "    A pipeline that applies text preprocessing then extracts keywords.\n",
    "    \"\"\"\n",
    "    text = normalize_pipeline(text)\n",
    "    doc = nlp(text)\n",
    "    tags = get_info_tags(doc)\n",
    "    \n",
    "    if return_doc:\n",
    "        return tags, doc\n",
    "    else:\n",
    "        return tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag evidences\n",
    "\n",
    "Creates the keyword-evidence index by matching keywords using POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_evidences(\n",
    "    evidence_path:Path,\n",
    "    save_path:Path = None,\n",
    "    processes:int = 8,\n",
    "    verbose:bool = True\n",
    ") -> DefaultDict[str, set]:\n",
    "    \n",
    "    # Load the evidence file\n",
    "    with open(evidence_path, mode=\"r\") as f:\n",
    "        evidences = json.load(f)\n",
    "    \n",
    "    # Cumulator\n",
    "    evidence_tags = defaultdict(set)\n",
    "    \n",
    "    evidences_iter = tqdm(evidences.items(), desc=\"claims\", disable=not verbose)\n",
    "    for evidence_id, evidence_text in evidences_iter:\n",
    "        \n",
    "        tags = info_tag_pipeline(text=evidence_text)\n",
    "        \n",
    "        for tag in tags:\n",
    "            evidence_tags[tag.tag].add(evidence_id)\n",
    "        \n",
    "        evidences_iter.postfix = f\"n_tags: {len(tags)}\"\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, mode=\"w\") as f:\n",
    "            json.dump(obj=evidence_tags, fp=f, cls=SetEncoder)\n",
    "            print(f\"saved to: {save_path}\")\n",
    "\n",
    "    return evidence_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_evidences_path = SAVE_PATH.with_name(\"tagged_evidences.json\")\n",
    "\n",
    "if tagged_evidences_path.exists():\n",
    "    print(f\"existing found: {tagged_evidences_path}\")\n",
    "    \n",
    "    # with open(tagged_evidences_path, mode=\"r\") as f:\n",
    "    #     tagged_evidences = json.load(f)\n",
    "    #     print(f\"loaded: {tagged_evidences_path}\")\n",
    "        \n",
    "else:\n",
    "    \n",
    "    tagged_evidences = tag_evidences(\n",
    "        evidence_path=DATA_PATH.with_name(\"evidence.json\"),\n",
    "        save_path=tagged_evidences_path\n",
    "    )\n",
    "# tagged_evidences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag evidences with char n-gram\n",
    "\n",
    "Create forward and reverse character n-grams to be used as keywords in the keyword-evidence index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bidirectional_n_grams(doc, n:int=4):\n",
    "    fwd_ngrams = [token.lemma_[:n] for token in doc if len(token.lemma_) >= n]\n",
    "    rev_ngrams = [token.lemma_[-n:] for token in doc if len(token.lemma_) >= n]\n",
    "    return fwd_ngrams, rev_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_evidences(\n",
    "    evidence_path:Path,\n",
    "    n_list:list = [4, 5, 6],\n",
    "    save_path_fwd:Path = None,\n",
    "    save_path_rev:Path = None,\n",
    "    verbose:bool = True\n",
    "):\n",
    "    # Load the evidence file\n",
    "    with open(evidence_path, mode=\"r\") as f:\n",
    "        evidences = json.load(f)\n",
    "    \n",
    "    # Cumulator\n",
    "    fwd_evidence_ngrams = defaultdict(set)\n",
    "    rev_evidence_ngrams = defaultdict(set)\n",
    "    \n",
    "    evidences_iter = tqdm(evidences.items(), desc=\"claims\", disable=not verbose)\n",
    "    for evidence_id, evidence_text in evidences_iter:\n",
    "        \n",
    "        text = normalize_pipeline(evidence_text)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        for n in n_list:\n",
    "            fwd_ngrams, rev_ngrams = get_bidirectional_n_grams(doc, n=n)\n",
    "            \n",
    "            for ngram in fwd_ngrams:\n",
    "                fwd_evidence_ngrams[ngram].add(evidence_id)\n",
    "                \n",
    "            for ngram in rev_ngrams:\n",
    "                rev_evidence_ngrams[ngram].add(evidence_id)\n",
    "            \n",
    "        continue\n",
    "    \n",
    "    if save_path_fwd and save_path_rev:\n",
    "        with open(save_path_fwd, mode=\"w\") as f:\n",
    "            json.dump(obj=fwd_evidence_ngrams, fp=f, cls=SetEncoder)\n",
    "            print(f\"saved to: {save_path_fwd}\")\n",
    "        with open(save_path_rev, mode=\"w\") as f:\n",
    "            json.dump(obj=rev_evidence_ngrams, fp=f, cls=SetEncoder)\n",
    "            print(f\"saved to: {save_path_rev}\")\n",
    "    \n",
    "    return fwd_evidence_ngrams, rev_evidence_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_ngram_evidences_path = SAVE_PATH.with_name(\"train_fwd_ngram_evidences.json\")\n",
    "rev_ngram_evidences_path = SAVE_PATH.with_name(\"train_rev_ngram_evidences.json\")\n",
    "\n",
    "if fwd_ngram_evidences_path.exists():\n",
    "    print(f\"existing found: {fwd_ngram_evidences_path}\")\n",
    "    \n",
    "    # with open(fwd_ngram_evidences_path, mode=\"r\") as f:\n",
    "    #     fwd_ngram_evidences = json.load(f)\n",
    "    #     print(f\"loaded: {fwd_ngram_evidences_path}\")\n",
    "        \n",
    "    # with open(rev_ngram_evidences_path, mode=\"r\") as f:\n",
    "    #     rev_ngram_evidences = json.load(f)\n",
    "    #     print(f\"loaded: {rev_ngram_evidences_path}\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    fwd_ngram_evidences, rev_ngram_evidences = get_ngram_evidences(\n",
    "        evidence_path=DATA_PATH.with_name(\"evidence.json\"),\n",
    "        n_list=[3, 4, 5, 6, 7, 8],\n",
    "        save_path_fwd=fwd_ngram_evidences_path,\n",
    "        save_path_rev=rev_ngram_evidences_path\n",
    "    )\n",
    "    \n",
    "# fwd_ngram_evidences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get evidence shortlist by claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidence_shortlist(\n",
    "    claims_paths:List[Path],\n",
    "    tagged_evidence_path:Path,\n",
    "    fwd_ngram_evidences_path:Path,\n",
    "    rev_ngram_evidences_path:Path,\n",
    "    ngram_list:list = [3, 4, 5, 6, 7, 8],\n",
    "    save_path:Path = None,\n",
    "    n_total_evidences:int=1208827,\n",
    "    max_retrieved:int=1000,\n",
    "    verbose:bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates evidence shortlists by matching keywords from a pre-computed\n",
    "    keyword-evidence index (tagged_evidence_path). Applies IDF scoring to\n",
    "    each matched keyword and incrementally increase the threshold for the\n",
    "    sum of IDF scores until the number of evidences lie within the\n",
    "    max_retrieved parameter (reported as the hyperparameter \"m\").\n",
    "    \"\"\"\n",
    "    # Load the claims file\n",
    "    claims = dict()\n",
    "    for claims_path in claims_paths:\n",
    "        with open(claims_path, mode=\"r\") as f:\n",
    "            claims.update(json.load(f))\n",
    "        \n",
    "    # Load the tagged evidences file\n",
    "    with open(tagged_evidence_path, mode=\"r\") as f:\n",
    "        tagged_evidences = json.load(f)\n",
    "    \n",
    "    # Load the evidence ngrams files\n",
    "    with open(fwd_ngram_evidences_path, mode=\"r\") as f:\n",
    "        fwd_ngram_evidences = json.load(f)\n",
    "        print(f\"loaded: {fwd_ngram_evidences_path}\")\n",
    "\n",
    "    with open(rev_ngram_evidences_path, mode=\"r\") as f:\n",
    "        rev_ngram_evidences = json.load(f)\n",
    "        print(f\"loaded: {rev_ngram_evidences_path}\")\n",
    "        \n",
    "    def idf_score(doc_freq, n_doc:int=n_total_evidences):\n",
    "        return np.log10(n_doc / doc_freq)\n",
    "    \n",
    "    # Cumulator\n",
    "    claim_evidences = defaultdict(set)\n",
    "    missed_retrievals = defaultdict(set)\n",
    "    retrieval_counts = []\n",
    "    retrieval_recalls = []\n",
    "    all_unique_tags = set()\n",
    "    \n",
    "    claim_obj = tqdm(claims.items(), desc=\"claims\", disable=not verbose)\n",
    "    for claim_id, claim in claim_obj:\n",
    "\n",
    "        # Get claim direct tags\n",
    "        tags = set()\n",
    "        claim_text = claim[\"claim_text\"]\n",
    "        claim_tags, claim_doc = info_tag_pipeline(text=claim_text, return_doc=True)\n",
    "        for tag in claim_tags:\n",
    "            tags.add(tag.tag)\n",
    "            all_unique_tags.add(tag.tag) #!\n",
    "        \n",
    "        original_tags = tags.copy() #!\n",
    "\n",
    "        # Keep a count of how many tags each evidence relates to\n",
    "        # In this version, Counters acts as summation for IDF scores\n",
    "        retrieved_evidence_counts = Counter()\n",
    "        \n",
    "        # Match tags -----------------------------------------------------\n",
    "        \n",
    "        # Go through all the tags then retrieve evidences\n",
    "        for tag in tags:\n",
    "            \n",
    "            evidence_ids = list(set(tagged_evidences.get(tag, [])))\n",
    "            \n",
    "            for e_id in evidence_ids:\n",
    "                retrieved_evidence_counts[e_id] += idf_score(len(evidence_ids))\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # Match ngrams ------------------------------------------------------\n",
    "        \n",
    "        # Ensure to match each token once, forwards and reverse, longest first\n",
    "        fwd_ngram_matched_tokens = set()\n",
    "        rev_ngram_matched_tokens = set()\n",
    "        \n",
    "        for token in claim_doc:\n",
    "            token_lemma = token.lemma_\n",
    "            \n",
    "            for n in sorted(ngram_list, reverse=True):\n",
    "                # Match forward and add IDF score\n",
    "                fwd_lemma = token_lemma[:n]\n",
    "                fwd_evidences = fwd_ngram_evidences.get(fwd_lemma, [])\n",
    "                \n",
    "                for e_id in fwd_evidences:\n",
    "                    retrieved_evidence_counts[e_id] += idf_score(len(fwd_evidences))\n",
    "                fwd_ngram_matched_tokens.add(token_lemma)\n",
    "                    \n",
    "                # Match reverse and add IDF score\n",
    "                rev_lemma = token_lemma[-n:0]\n",
    "                rev_evidences = rev_ngram_evidences.get(rev_lemma, [])\n",
    "                \n",
    "                for e_id in rev_evidences:\n",
    "                    retrieved_evidence_counts[e_id] += idf_score(len(rev_evidences))\n",
    "                rev_ngram_matched_tokens.add(token_lemma)\n",
    "        \n",
    "        # Wrapping up ------------------------------------------------------\n",
    "            \n",
    "        # Dynamically find the cutoff to return the maximum specified\n",
    "        # number of evidences\n",
    "        if len(retrieved_evidence_counts.keys()) > 1:\n",
    "            retrieved_cut_cutoff = 1\n",
    "            staged_retrievals = []\n",
    "            searching_cutoff = True\n",
    "            while searching_cutoff:\n",
    "                current_staged_retrievals = [\n",
    "                    e_id\n",
    "                    for e_id, e_count in sorted(\n",
    "                        retrieved_evidence_counts.items(),\n",
    "                        key=lambda x: x[1], reverse=True\n",
    "                    )\n",
    "                    if e_count >= retrieved_cut_cutoff\n",
    "                ]\n",
    "                \n",
    "                if len(current_staged_retrievals) < 1:\n",
    "                    searching_cutoff = False\n",
    "                \n",
    "                staged_retrievals = current_staged_retrievals\n",
    "                \n",
    "                if len(current_staged_retrievals) > max_retrieved:\n",
    "                    retrieved_cut_cutoff += 0.5\n",
    "                    continue\n",
    "            \n",
    "                searching_cutoff = False\n",
    "            \n",
    "            # Add staged retrievals against the claim_id\n",
    "            for e_id in staged_retrievals:\n",
    "                claim_evidences[claim_id].add(e_id)\n",
    "\n",
    "        \n",
    "        # Count how many evidences have been retrieved for this claim\n",
    "        n_retrieved = len(claim_evidences[claim_id])\n",
    "        \n",
    "        # Calculate some statistics\n",
    "        recall = 1\n",
    "        if \"evidences\" in claim.keys():\n",
    "            truth_evidences = set(claim[\"evidences\"])\n",
    "            retrieved_evidences = set(claim_evidences[claim_id])\n",
    "            missed = truth_evidences.difference(retrieved_evidences)\n",
    "            recall = (len(truth_evidences) - len(missed)) / len(truth_evidences)\n",
    "            \n",
    "            if recall < 0.5:\n",
    "                print(claim_id, recall, original_tags)\n",
    "                \n",
    "            missed_retrievals[claim_id].update(missed)\n",
    "        \n",
    "        retrieval_counts.append(n_retrieved)\n",
    "        retrieval_recalls.append(recall)\n",
    "        claim_obj.postfix = f\"n_retrieved: {n_retrieved}, recall: {recall}\"\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, mode=\"w\") as f:\n",
    "            json.dump(obj=claim_evidences, fp=f, cls=SetEncoder)\n",
    "            print(f\"saved to: {save_path}\")\n",
    "    \n",
    "    return claim_evidences, missed_retrievals, retrieval_counts, retrieval_recalls, all_unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retrieval = 1000\n",
    "retrieved_evidences_path = SAVE_PATH.with_name(f\"test_shortlist_evidences_max_{max_retrieval}.json\")\n",
    "\n",
    "if retrieved_evidences_path.exists():\n",
    "    \n",
    "    with open(retrieved_evidences_path, mode=\"r\") as f:\n",
    "        retrieved_evidences = json.load(f)\n",
    "        print(f\"loaded: {retrieved_evidences_path}\")\n",
    "        \n",
    "else:\n",
    "    \n",
    "    retrieved_evidences, missed_retrievals, retrievals_counts, retrieval_recalls, all_unique_tags = \\\n",
    "    get_evidence_shortlist(\n",
    "        claims_paths=[DATA_PATH.with_name(\"test-claims-unlabelled.json\")],\n",
    "        tagged_evidence_path=tagged_evidences_path,\n",
    "        fwd_ngram_evidences_path=fwd_ngram_evidences_path,\n",
    "        rev_ngram_evidences_path=rev_ngram_evidences_path,\n",
    "        ngram_list=[4, 5, 6, 7, 8],\n",
    "        save_path=retrieved_evidences_path,\n",
    "        max_retrieved=max_retrieval,\n",
    "    )\n",
    "# retrieved_evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"avg recall: {np.mean(retrieval_recalls)}\")\n",
    "print(f\"min recall: {np.min(retrieval_recalls)}\")\n",
    "print(f\"avg retrieved: {np.mean(retrievals_counts)}\")\n",
    "print(f\"max retrieved: {np.max(retrievals_counts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp90042_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
