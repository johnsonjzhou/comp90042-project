{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 06a Robert MNLI Cross Entropy Classification for Label Prediction\n",
    "\n",
    "Prediction of claim labels based on the matched evidence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to project root\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT_DIR = Path.cwd()\n",
    "while not ROOT_DIR.joinpath(\"src\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device is 'mps'\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score\n",
    "\n",
    "from src.logger import SimpleLogger\n",
    "from src.model_05 import BertCrossEncoderClassifier\n",
    "from src.model_05 import RobertaLargeCrossEncoderClassifier\n",
    "from src.data import LabelClassificationDataset\n",
    "from src.torch_utils import get_torch_device\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import exp\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "TORCH_DEVICE = get_torch_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ROOT_DIR.joinpath(\"./result/models/*\")\n",
    "DATA_PATH = ROOT_DIR.joinpath(\"./data/*\")\n",
    "LOG_PATH = ROOT_DIR.joinpath(\"./result/logs/*\")\n",
    "SHORTLIST_PATH = ROOT_DIR.joinpath(\"./result/pipeline/shortlisting_v2/*\")\n",
    "\n",
    "run_time = datetime.now().strftime('%Y_%m_%d_%H_%M')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    model,\n",
    "    claims_paths:List[Path],\n",
    "    save_path:Path=None,\n",
    "    label_weight:list=None,\n",
    "    label_smoothing:float=None,\n",
    "    warmup:float=0.1,\n",
    "    lr:float=0.00005, # 5e-5\n",
    "    weight_decay:float=0.01,\n",
    "    normalize_text:bool=True,\n",
    "    max_length:int=128,\n",
    "    dropout:float=None,\n",
    "    n_epochs:int=5,\n",
    "    batch_size:int=64,\n",
    "):\n",
    "    # Generate training dataset\n",
    "    train_data = LabelClassificationDataset(\n",
    "        claims_paths=claims_paths,\n",
    "        training=True,\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Generate evaluation dataset\n",
    "    dev_data = LabelClassificationDataset(\n",
    "        claims_paths=[Path(\"./data/dev-claims.json\")],\n",
    "        training=True,\n",
    "    )\n",
    "    dev_dataloader = DataLoader(\n",
    "        dataset=dev_data,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    loss_fn = CrossEntropyLoss(\n",
    "        weight=torch.tensor(label_weight, device=TORCH_DEVICE),\n",
    "        label_smoothing=label_smoothing\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(\n",
    "        params=model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler = LinearLR(\n",
    "        optimizer=optimizer,\n",
    "        total_iters=warmup * len(train_dataloader),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy_fn = MulticlassAccuracy()\n",
    "    f1_fn = MulticlassF1Score()\n",
    "    \n",
    "    # Training epochs --------------------------------------------------------\n",
    "    \n",
    "    best_epoch_loss = 999\n",
    "    best_epoch_f1 = -1\n",
    "    best_epoch_acc = -1\n",
    "    best_epoch = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print(f\"\\nEpoch: {epoch + 1} of {n_epochs}\\n\")\n",
    "        \n",
    "        # Run training -------------------------------------------------------\n",
    "        model.train()\n",
    "        \n",
    "        train_batches = tqdm(train_dataloader, desc=\"train batches\")\n",
    "        running_losses = []\n",
    "        for batch in train_batches:\n",
    "            claim_texts, evidence_texts, labels, claim_ids, evidence_ids = batch\n",
    "            texts = list(zip(claim_texts, evidence_texts))\n",
    "            \n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + loss\n",
    "            output, logits, seq = model(\n",
    "                texts=texts,\n",
    "                normalize_text=normalize_text,\n",
    "                max_length=max_length,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            # Backward + optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            batch_loss = loss.item() * len(batch)\n",
    "            running_losses.append(batch_loss)\n",
    "            \n",
    "            train_batches.postfix = f\"loss: {batch_loss:.3f}\"\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        # Epoch loss\n",
    "        epoch_loss = np.average(running_losses)\n",
    "        print(f\"Average epoch loss: {epoch_loss:.3f}\")\n",
    "    \n",
    "        # Run evaluation ------------------------------------------------------\n",
    "        model.eval()\n",
    "\n",
    "        dev_batches = tqdm(dev_dataloader, desc=\"dev batches\")\n",
    "        dev_acc = []\n",
    "        dev_f1 = []\n",
    "        for batch in dev_batches:\n",
    "            claim_texts, evidence_texts, labels, claim_ids, evidence_ids = batch\n",
    "            texts = list(zip(claim_texts, evidence_texts))\n",
    "\n",
    "            # Forward\n",
    "            output, logits, seq = model(\n",
    "                texts=texts,\n",
    "                normalize_text=normalize_text,\n",
    "                max_length=max_length,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            \n",
    "            # Prediction\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "\n",
    "            # Metrics\n",
    "            accuracy_fn.update(predicted.cpu(), labels.cpu())\n",
    "            f1_fn.update(predicted.cpu(), labels.cpu())\n",
    "            \n",
    "            acc = accuracy_fn.compute()\n",
    "            f1 = f1_fn.compute()\n",
    "            \n",
    "            dev_acc.append(acc)\n",
    "            dev_f1.append(f1)\n",
    "            \n",
    "            dev_batches.postfix = f\" acc: {acc:.3f}, f1: {f1:.3f}\"\n",
    "\n",
    "            continue\n",
    "        \n",
    "        # Consider metrics\n",
    "        epoch_acc = np.average(dev_acc)\n",
    "        print(f\"Average epoch accuracy: {epoch_acc:.3f}\")\n",
    "        \n",
    "        epoch_f1 = np.average(dev_f1)\n",
    "        print(f\"Average epoch f1: {epoch_f1:.3f}\")\n",
    "        \n",
    "        if epoch_acc > best_epoch_acc:\n",
    "            best_epoch_acc = epoch_acc\n",
    "        \n",
    "        if epoch_f1 > best_epoch_f1:\n",
    "            best_epoch_f1 = epoch_f1\n",
    "            best_epoch = epoch + 1\n",
    "        \n",
    "        # Save model ----------------------------------------------------------\n",
    "        \n",
    "        # Save the model with the best f1 score\n",
    "        if save_path and epoch_f1 >= best_epoch_f1:\n",
    "            torch.save(model, save_path)\n",
    "            print(f\"Saved model to: {save_path}\")\n",
    "        \n",
    "    print(\"Done!\")\n",
    "    return best_epoch_acc, best_epoch_f1, best_epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a blank pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = RobertaLargeCrossEncoderClassifier(\n",
    "    pretrained_name=\"roberta-large-mnli\",\n",
    "    n_classes=3,\n",
    "    device=TORCH_DEVICE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load one previously trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_SAVE_PATH = MODEL_PATH.with_name(\"\")\n",
    "# with open(MODEL_PATH.with_name(MODEL_SAVE_PATH), mode=\"rb\") as f:\n",
    "#     model = torch.load(f, map_location=TORCH_DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 1228/1228 [00:00<00:00, 461647.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=3730\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 518397.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=433\n",
      "\n",
      "Epoch: 1 of 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 234/234 [10:51<00:00,  2.79s/it, loss: 1.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 3.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 28/28 [01:31<00:00,  3.26s/it,  acc: 0.691, f1: 0.691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch accuracy: 0.666\n",
      "Average epoch f1: 0.666\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_06a_roberta_mnli_cross_encoder_label_2023_05_11_17_47.pth\n",
      "\n",
      "Epoch: 2 of 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 234/234 [11:30<00:00,  2.95s/it, loss: 9.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 2.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s,  acc: 0.679, f1: 0.679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch accuracy: 0.682\n",
      "Average epoch f1: 0.682\n",
      "Saved model to: /Users/johnsonzhou/git/comp90042-project/result/models/model_06a_roberta_mnli_cross_encoder_label_2023_05_11_17_47.pth\n",
      "\n",
      "Epoch: 3 of 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 234/234 [11:37<00:00,  2.98s/it, loss: 0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 1.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 28/28 [00:23<00:00,  1.21it/s,  acc: 0.682, f1: 0.682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch accuracy: 0.681\n",
      "Average epoch f1: 0.681\n",
      "\n",
      "Epoch: 4 of 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 234/234 [11:37<00:00,  2.98s/it, loss: 0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 1.149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s,  acc: 0.684, f1: 0.684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch accuracy: 0.682\n",
      "Average epoch f1: 0.682\n",
      "\n",
      "Epoch: 5 of 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 234/234 [11:36<00:00,  2.98s/it, loss: 0.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 28/28 [00:22<00:00,  1.24it/s,  acc: 0.681, f1: 0.681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch accuracy: 0.682\n",
      "Average epoch f1: 0.682\n",
      "\n",
      "Epoch: 6 of 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 234/234 [11:25<00:00,  2.93s/it, loss: 0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 0.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 28/28 [00:21<00:00,  1.29it/s,  acc: 0.682, f1: 0.682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch accuracy: 0.681\n",
      "Average epoch f1: 0.681\n",
      "\n",
      "Epoch: 7 of 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches: 100%|██████████| 234/234 [11:01<00:00,  2.83s/it, loss: 1.381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch loss: 0.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev batches: 100%|██████████| 28/28 [00:21<00:00,  1.31it/s,  acc: 0.682, f1: 0.682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch accuracy: 0.681\n",
      "Average epoch f1: 0.681\n",
      "\n",
      "Epoch: 8 of 15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train batches:  35%|███▍      | 81/234 [03:52<07:18,  2.87s/it, loss: 0.842]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_loop(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      3\u001b[0m     claims_paths\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      4\u001b[0m         DATA_PATH\u001b[39m.\u001b[39;49mwith_name(\u001b[39m\"\u001b[39;49m\u001b[39mtrain-claims.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m     ],\n\u001b[1;32m      6\u001b[0m     save_path\u001b[39m=\u001b[39;49mMODEL_PATH\u001b[39m.\u001b[39;49mwith_name(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel_06a_roberta_mnli_cross_encoder_label_\u001b[39;49m\u001b[39m{\u001b[39;49;00mrun_time\u001b[39m}\u001b[39;49;00m\u001b[39m.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      7\u001b[0m     warmup\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m0.000005\u001b[39;49m, \u001b[39m# 5e-6\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     normalize_text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     dropout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     n_epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     \u001b[39m# label_weight=[2, 1.2, 1],\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m     label_weight\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m0.6\u001b[39;49m, \u001b[39m0.4\u001b[39;49m],\n\u001b[1;32m     16\u001b[0m     label_smoothing\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m )\n",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, claims_paths, save_path, label_weight, label_smoothing, warmup, lr, weight_decay, normalize_text, max_length, dropout, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m# Backward + optimizer\u001b[39;00m\n\u001b[1;32m     94\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 95\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     97\u001b[0m \u001b[39m# Update running loss\u001b[39;00m\n\u001b[1;32m     98\u001b[0m batch_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(batch)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     adamw(\n\u001b[1;32m    172\u001b[0m         params_with_grad,\n\u001b[1;32m    173\u001b[0m         grads,\n\u001b[1;32m    174\u001b[0m         exp_avgs,\n\u001b[1;32m    175\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    176\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    177\u001b[0m         state_steps,\n\u001b[1;32m    178\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    179\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    180\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    181\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    186\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    188\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    189\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m func(\n\u001b[1;32m    322\u001b[0m     params,\n\u001b[1;32m    323\u001b[0m     grads,\n\u001b[1;32m    324\u001b[0m     exp_avgs,\n\u001b[1;32m    325\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    326\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    327\u001b[0m     state_steps,\n\u001b[1;32m    328\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    329\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    330\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    331\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    332\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    333\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    334\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    335\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    336\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    337\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    338\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/torch/optim/adamw.py:440\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    438\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    439\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    442\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loop(\n",
    "    model=model,\n",
    "    claims_paths=[\n",
    "        DATA_PATH.with_name(\"train-claims.json\")\n",
    "    ],\n",
    "    save_path=MODEL_PATH.with_name(f\"model_06a_roberta_mnli_cross_encoder_label_{run_time}.pth\"),\n",
    "    warmup=0.1,\n",
    "    lr=0.000005, # 5e-6\n",
    "    weight_decay=0.02,\n",
    "    normalize_text=True,\n",
    "    max_length=512,\n",
    "    dropout=0.1,\n",
    "    n_epochs=15,\n",
    "    # label_weight=[2, 1.2, 1],\n",
    "    label_weight=[1, 0.6, 0.4],\n",
    "    label_smoothing=0.0,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams = ParameterGrid(param_grid={\n",
    "#     \"claims_paths\": [[\n",
    "#         DATA_PATH.with_name(\"train-claims.json\")\n",
    "#     ]],\n",
    "#     \"warmup\": [0.1],\n",
    "#     \"lr\": [0.000005],\n",
    "#     \"weight_decay\": [0.02],\n",
    "#     \"normalize_text\": [True],\n",
    "#     \"max_length\": [512],\n",
    "#     \"dropout\": [0.1],\n",
    "#     \"n_epochs\": [10],\n",
    "#     \"batch_size\": [24],\n",
    "#     \"freeze_bert\": [False],\n",
    "#     \"label_weight\":[\n",
    "#         # [2, 1.2, 1],\n",
    "#         [1, 0.6, 0.4],\n",
    "#     ],\n",
    "#     \"label_smoothing\": [0.0]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SimpleLogger(\"model_06_cross_encoder_retrieval\") as logger:\n",
    "    logger.set_stream_handler()\n",
    "    logger.set_file_handler(\n",
    "        log_path=LOG_PATH,\n",
    "        filename=\"model_06_hyperparam_tuning.txt\"\n",
    "    )\n",
    "    best_f1 = -1\n",
    "    best_params = {}\n",
    "    for hyperparam in hyperparams:\n",
    "        model = BertCrossEncoderClassifier(\n",
    "            pretrained_name=\"bert-base-uncased\",\n",
    "            n_classes=3,\n",
    "            device=TORCH_DEVICE\n",
    "        )\n",
    "        \n",
    "        model_param = hyperparam.copy()\n",
    "        \n",
    "        # Freeze bert parameters if desired\n",
    "        if \"freeze_bert\" in model_param.keys():\n",
    "            if hyperparam[\"freeze_bert\"] is True:\n",
    "                for param in model.bert.parameters():\n",
    "                    param.requires_grad = False\n",
    "            del model_param[\"freeze_bert\"]\n",
    "        \n",
    "        logger.info(\"\\n== RUN\")\n",
    "        logger.info(hyperparam)\n",
    "        \n",
    "        accuracy, f1, epoch = training_loop(model=model, **model_param)\n",
    "        \n",
    "        logger.info(f\"run_best_epoch: {epoch}, run_best_acc: {accuracy}, run_best_f1: {f1}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_params = hyperparam\n",
    "        \n",
    "        logger.info(f\"\\n== CURRENT BEST F1: {best_f1}\")\n",
    "        logger.info(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp90042_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
