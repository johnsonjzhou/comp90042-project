{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 03 inference\n",
    "\n",
    "Evidence retrieval using a siamese BERT classification model. This is similar to Model 01 except this does not use any community based models from sentence transformer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to project root\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT_DIR = Path.cwd()\n",
    "while not ROOT_DIR.joinpath(\"src\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ROOT_DIR.joinpath(\"./result/models/*\")\n",
    "OUTPUT_PATH = ROOT_DIR.joinpath(\"./result/inference/*\")\n",
    "DATA_PATH = ROOT_DIR.joinpath(\"./data/*\")\n",
    "NER_PATH = ROOT_DIR.joinpath(\"./result/ner/*\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device is 'mps'\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Linear, Module, CrossEntropyLoss, Dropout, CosineSimilarity\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torch.nn.functional import relu, softmax\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryRecall\n",
    "from sentence_transformers import util\n",
    "\n",
    "from src.torch_utils import get_torch_device\n",
    "from src.data import create_claim_output\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import exp\n",
    "from collections import defaultdict\n",
    "\n",
    "TORCH_DEVICE = get_torch_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClaimEvidencePair:\n",
    "    claim_id:str\n",
    "    evidence_id:str\n",
    "    label:int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceClaims(Dataset):\n",
    "    \n",
    "    def __init__(self, claims_path:Path) -> None:\n",
    "        super(InferenceClaims, self).__init__()\n",
    "        with open(claims_path, mode=\"r\") as f:\n",
    "            self.claims = json.load(fp=f)\n",
    "            self.claim_ids = list(self.claims.keys())\n",
    "            print(f\"loaded inference claims n={len(self.claim_ids)}\")\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.claim_ids)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[str]:\n",
    "        claim_id = self.claim_ids[idx]\n",
    "        claim_text = self.claims[claim_id][\"claim_text\"]\n",
    "        return claim_id, claim_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeenOnlyDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        claim_id:str,\n",
    "        claims_path:Path,\n",
    "        train_claims_paths:List[Path],\n",
    "        evidence_path:Path,\n",
    "        verbose:bool=True\n",
    "    ) -> None:\n",
    "        super(SeenOnlyDataset, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.claim_id = claim_id\n",
    "        \n",
    "        # Load claims text from json\n",
    "        with open(claims_path, mode=\"r\") as f:\n",
    "            claims = json.load(fp=f)\n",
    "            self.claim_text = claims[self.claim_id][\"claim_text\"]\n",
    "            \n",
    "        # Load evidence library\n",
    "        with open(evidence_path, mode=\"r\") as f:\n",
    "            self.evidence = json.load(fp=f)\n",
    "        \n",
    "        # Load training claims\n",
    "        train_claims = dict()\n",
    "        for path in train_claims_paths:\n",
    "            with open(path, mode=\"r\") as f:\n",
    "                train_claims.update(json.load(f))\n",
    "        \n",
    "        # Load the evidence shortlist\n",
    "        evidence_shortlist = set()\n",
    "        for claim in train_claims.values():\n",
    "            evidence_shortlist.update(claim[\"evidences\"])\n",
    "        self.evidence_shortlist = sorted(evidence_shortlist)\n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.evidence_shortlist)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[str]:\n",
    "        # Get text ids\n",
    "        claim_id = self.claim_id\n",
    "        evidence_id = self.evidence_shortlist[idx]\n",
    "        \n",
    "        # Get text\n",
    "        claim_text = self.claim_text\n",
    "        evidence_text = self.evidence[evidence_id]\n",
    "\n",
    "        return (claim_text, evidence_text, claim_id, evidence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDatasetInference(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        claim_id:str,\n",
    "        claims_path:Path,\n",
    "        evidence_path:Path,\n",
    "        claims_shortlist:Path = None,\n",
    "        evidence_shortlist:Path = None,\n",
    "        verbose:bool=True\n",
    "    ) -> None:\n",
    "        super(SiameseDatasetInference, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.claim_id = claim_id\n",
    "        \n",
    "        # Load claims text from json\n",
    "        with open(claims_path, mode=\"r\") as f:\n",
    "            claims = json.load(fp=f)\n",
    "            self.claim_text = claims[self.claim_id][\"claim_text\"]\n",
    "            \n",
    "        # Load evidence library\n",
    "        with open(evidence_path, mode=\"r\") as f:\n",
    "            self.evidence = json.load(fp=f)\n",
    "        \n",
    "        # Load the pre-retrieved shortlist of evidences\n",
    "        # From either a pre-retrieved list of evidences specific for the\n",
    "        # claim_id or from a pre-collated evidence shortlist\n",
    "        # Both of which were determined from the fast shortlisting process\n",
    "        if claims_shortlist is not None:\n",
    "            with open(claims_shortlist, mode=\"r\") as f:\n",
    "                claims_shortlist_ = json.load(fp=f)\n",
    "                self.evidence_shortlist = list(set(\n",
    "                    claims_shortlist_.get(self.claim_id, [])\n",
    "                ))\n",
    "                print(f\"loaded claims_shortlist: {claims_shortlist}\")\n",
    "        elif evidence_shortlist is not None:\n",
    "            with open(evidence_shortlist, mode=\"r\") as f:\n",
    "                self.evidence_shortlist = list(set(json.load(fp=f)))\n",
    "                print(f\"loaded evidence_shortlist: {evidence_shortlist}\")\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Provide either a claims_shortlist or evidence_shortlist\"\n",
    "            )\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.evidence_shortlist)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[str]:\n",
    "        # Get text ids\n",
    "        claim_id = self.claim_id\n",
    "        evidence_id = self.evidence_shortlist[idx]\n",
    "        \n",
    "        # Get text\n",
    "        claim_text = self.claim_text\n",
    "        evidence_text = self.evidence[evidence_id]\n",
    "\n",
    "        return (claim_text, evidence_text, claim_id, evidence_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTripletEmbedderBert(Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_name:str,\n",
    "            device,\n",
    "            **kwargs\n",
    "        ) -> None:\n",
    "        super(SiameseTripletEmbedderBert, self).__init__(**kwargs)\n",
    "        self.device = device\n",
    "        \n",
    "        # Use a pretrained tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_name)\n",
    "        \n",
    "        # Use a pretrained model\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name)\n",
    "        self.bert.to(device=device)\n",
    "        return\n",
    "        \n",
    "    def forward(self, anchor_texts, pos_texts, neg_texts=None) -> Tuple[torch.Tensor]:\n",
    "        \n",
    "        # Run the tokenizer\n",
    "        t_kwargs = {\n",
    "            \"return_tensors\": \"pt\",\n",
    "            \"padding\": True,\n",
    "            \"truncation\": True,\n",
    "            \"max_length\": 100,\n",
    "            \"add_special_tokens\":True\n",
    "        }\n",
    "        anchor_x = self.tokenizer(anchor_texts, **t_kwargs)\n",
    "        pos_x = self.tokenizer(pos_texts, **t_kwargs)\n",
    "        if neg_texts:\n",
    "            neg_x = self.tokenizer(neg_texts, **t_kwargs)\n",
    "        \n",
    "        anchor_x = anchor_x[\"input_ids\"].to(device=self.device)\n",
    "        pos_x = pos_x[\"input_ids\"].to(device=self.device)\n",
    "        if neg_texts:\n",
    "            neg_x = neg_x[\"input_ids\"].to(device=self.device)\n",
    "        \n",
    "        # Run Bert\n",
    "        anchor_x = self.bert(anchor_x, return_dict=True).pooler_output\n",
    "        pos_x = self.bert(pos_x, return_dict=True).pooler_output\n",
    "        if neg_texts:\n",
    "            neg_x = self.bert(neg_x, return_dict=True).pooler_output\n",
    "        # dim=768\n",
    "        \n",
    "        if neg_texts:\n",
    "            return anchor_x, pos_x, neg_x\n",
    "        else:\n",
    "            return anchor_x, pos_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = f\"model_03a_bert_base_triplet_margin_2_neg_5_10epochs.pth\"\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL_PATH.with_name(MODEL_NAME), mode=\"rb\") as f:\n",
    "    model = torch.load(f, map_location=TORCH_DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvidenceScore:\n",
    "    evidence_id:str\n",
    "    score:float=0\n",
    "    cos_sim:float=0\n",
    "    dot:float=0\n",
    "    \n",
    "    def to_list(self) -> List[str]:\n",
    "        return [self.evidence_id, str(self.score)]\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, str]:\n",
    "        return {\"evidence_id\": self.evidence_id, \"score\": str(self.score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate claims-evidence inference interations\n",
    "infer_data = SeenOnlyDataset(\n",
    "    claim_id=\"claim-1937\",\n",
    "    claims_path=DATA_PATH.with_name(\"train-claims.json\"),\n",
    "    evidence_path=DATA_PATH.with_name(\"evidence.json\"),\n",
    "    train_claims_paths=[\n",
    "        DATA_PATH.with_name(\"dev-claims.json\"),\n",
    "        DATA_PATH.with_name(\"train-claims.json\")\n",
    "    ],\n",
    ")\n",
    "infer_dataloader = DataLoader(\n",
    "    dataset=infer_data,\n",
    "    shuffle=False,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 54/54 [00:16<00:00,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set model mode to evaluation\n",
    "model.eval()\n",
    "\n",
    "# Cumulator\n",
    "claim_predictions = []\n",
    "infer_batches = tqdm(infer_dataloader, desc=\"infer batches\")\n",
    "for batch in infer_batches:\n",
    "    claim_texts, evidence_texts, batch_claim_ids, batch_evidence_ids = batch\n",
    "    \n",
    "    # # Forward\n",
    "    claim_emb, evidence_emb = model(claim_texts, evidence_texts)\n",
    "    \n",
    "    for e_id, c, e in zip(\n",
    "        batch_evidence_ids,\n",
    "        claim_emb,\n",
    "        evidence_emb\n",
    "    ):\n",
    "        # Score embeddings with cosine similarity\n",
    "        cos_sim_func = CosineSimilarity(dim=-1)\n",
    "        cos_sim = cos_sim_func(c, e).detach().item()\n",
    "        \n",
    "        # Dot product\n",
    "        dot = util.dot_score(c, e).squeeze().detach().item()\n",
    "        \n",
    "        claim_predictions.append(EvidenceScore(\n",
    "            evidence_id=e_id,\n",
    "            cos_sim=cos_sim,\n",
    "            dot=dot\n",
    "        ))\n",
    "    \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455 EvidenceScore(evidence_id='evidence-442946', score=0, cos_sim=0.941272497177124, dot=339.094482421875)\n"
     ]
    }
   ],
   "source": [
    "for i, predicted_evidence in enumerate(sorted(claim_predictions, key=lambda e: e.cos_sim, reverse=True)):\n",
    "    if predicted_evidence.evidence_id == \"evidence-442946\":\n",
    "        print(i, predicted_evidence)\n",
    "        break\n",
    "    # print(predicted_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 EvidenceScore(evidence_id='evidence-141844', score=0, cos_sim=0.9891319274902344, dot=383.96307373046875)\n",
      "1 EvidenceScore(evidence_id='evidence-1180647', score=0, cos_sim=0.9867583513259888, dot=392.5845947265625)\n",
      "2 EvidenceScore(evidence_id='evidence-140540', score=0, cos_sim=0.985182523727417, dot=374.7574157714844)\n",
      "3 EvidenceScore(evidence_id='evidence-433622', score=0, cos_sim=0.9834713935852051, dot=368.0030822753906)\n",
      "4 EvidenceScore(evidence_id='evidence-457889', score=0, cos_sim=0.9834628701210022, dot=376.37432861328125)\n",
      "5 EvidenceScore(evidence_id='evidence-892616', score=0, cos_sim=0.9826794862747192, dot=371.2298889160156)\n",
      "6 EvidenceScore(evidence_id='evidence-914173', score=0, cos_sim=0.9817409515380859, dot=376.58978271484375)\n",
      "7 EvidenceScore(evidence_id='evidence-1069909', score=0, cos_sim=0.9815636277198792, dot=381.1744384765625)\n",
      "8 EvidenceScore(evidence_id='evidence-754568', score=0, cos_sim=0.9813380837440491, dot=371.28375244140625)\n",
      "9 EvidenceScore(evidence_id='evidence-719544', score=0, cos_sim=0.9800635576248169, dot=379.2900695800781)\n",
      "10 EvidenceScore(evidence_id='evidence-518962', score=0, cos_sim=0.9800459146499634, dot=372.0763244628906)\n",
      "11 EvidenceScore(evidence_id='evidence-733280', score=0, cos_sim=0.9800118207931519, dot=375.5029296875)\n",
      "12 EvidenceScore(evidence_id='evidence-349143', score=0, cos_sim=0.9797239303588867, dot=374.7271423339844)\n",
      "13 EvidenceScore(evidence_id='evidence-832334', score=0, cos_sim=0.9795048236846924, dot=390.1274108886719)\n",
      "14 EvidenceScore(evidence_id='evidence-1091781', score=0, cos_sim=0.9793652892112732, dot=373.1499328613281)\n",
      "15 EvidenceScore(evidence_id='evidence-103287', score=0, cos_sim=0.9791512489318848, dot=363.92340087890625)\n",
      "16 EvidenceScore(evidence_id='evidence-779263', score=0, cos_sim=0.9791511297225952, dot=364.08172607421875)\n",
      "17 EvidenceScore(evidence_id='evidence-195369', score=0, cos_sim=0.9790983200073242, dot=385.83795166015625)\n",
      "18 EvidenceScore(evidence_id='evidence-609211', score=0, cos_sim=0.9789546728134155, dot=375.4394226074219)\n",
      "19 EvidenceScore(evidence_id='evidence-545691', score=0, cos_sim=0.9785110950469971, dot=368.0686950683594)\n"
     ]
    }
   ],
   "source": [
    "for i, predicted_evidence in enumerate(sorted(claim_predictions, key=lambda e: e.cos_sim, reverse=True)):\n",
    "    if i >= 20:\n",
    "        break\n",
    "    print(i, predicted_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp90042_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
