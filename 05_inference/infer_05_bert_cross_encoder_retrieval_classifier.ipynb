{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 05 inference\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to project root\n",
    "from pathlib import Path\n",
    "import os\n",
    "ROOT_DIR = Path.cwd()\n",
    "while not ROOT_DIR.joinpath(\"src\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "os.chdir(ROOT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/comp90042_project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device is 'mps'\n"
     ]
    }
   ],
   "source": [
    "# Imports and dependencies\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score\n",
    "\n",
    "from src.torch_utils import get_torch_device\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Union, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from src.model_05 import BertCrossEncoderClassifier\n",
    "from src.data import RetrievalWithShortlistDataset, RetrievalDevEvalDataset, \\\n",
    "    InferenceClaims\n",
    "from src.logger import SimpleLogger\n",
    "\n",
    "TORCH_DEVICE = get_torch_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ROOT_DIR.joinpath(\"./result/models/*\")\n",
    "DATA_PATH = ROOT_DIR.joinpath(\"./data/*\")\n",
    "LOG_PATH = ROOT_DIR.joinpath(\"./result/logs/*\")\n",
    "SHORTLIST_PATH = ROOT_DIR.joinpath(\"./result/pipeline/shortlisting_v2/*\")\n",
    "OUTPUT_PATH = ROOT_DIR.joinpath(\"./result/pipeline/retrieval_classif/*\")\n",
    "\n",
    "run_time = datetime.now().strftime('%Y_%m_%d_%H_%M')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a blank pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertCrossEncoderClassifier(\n",
    "#     pretrained_name=\"bert-base-uncased\",\n",
    "#     n_classes=2,\n",
    "#     device=TORCH_DEVICE\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = MODEL_PATH.with_name(\"model_05_bert_cross_encoder_retrieval_2023_05_08_17_06.pth\")\n",
    "with open(MODEL_SAVE_PATH, mode=\"rb\") as f:\n",
    "    model = torch.load(f, map_location=TORCH_DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvidenceScore:\n",
    "    evidence_id:str\n",
    "    score:float\n",
    "    \n",
    "    def to_list(self) -> List[str]:\n",
    "        return [self.evidence_id, str(self.score)]\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, str]:\n",
    "        return {\"evidence_id\": self.evidence_id, \"score\": str(self.score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_run(\n",
    "    claims_path:Path,\n",
    "    evidence_path:Path,\n",
    "    claims_shortlist_path:Path = None,\n",
    "    save_path:Path = None,\n",
    "    batch_size:int = 64\n",
    "):\n",
    "    # Generate claims iterations\n",
    "    inference_claims = InferenceClaims(claims_path=claims_path)\n",
    "    \n",
    "    # Cumulator\n",
    "    claim_predictions = dict()\n",
    "    \n",
    "    # Load from save to continue inference if exists\n",
    "    saved_predictions = dict()\n",
    "    if save_path.exists():\n",
    "        with open(save_path, mode=\"r\") as f:\n",
    "            saved_predictions.update(json.load(fp=f))\n",
    "    \n",
    "    for claim_id, claim_text in inference_claims:\n",
    "        \n",
    "        # Skip if a save entry exists\n",
    "        if claim_id in saved_predictions.keys():\n",
    "            print(f\"skipping {claim_id}, done previously\")\n",
    "            continue\n",
    "        \n",
    "        # Create a save entry\n",
    "        claim_predictions[claim_id] = {\n",
    "            \"claim_text\": claim_text,\n",
    "            \"evidences\": []\n",
    "        }\n",
    "        \n",
    "        # Generate claims-evidence inference interations\n",
    "        infer_data = RetrievalWithShortlistDataset(\n",
    "            claim_id=claim_id,\n",
    "            claims_paths=[claims_path],\n",
    "            claims_shortlist_paths=[claims_shortlist_path],\n",
    "            evidence_path=evidence_path,\n",
    "            pos_label=0, # Wont' matter what label we use for inference\n",
    "            neg_label=0,\n",
    "            n_neg_samples=9999999999,\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            verbose=True\n",
    "        )\n",
    "        infer_dataloader = DataLoader(\n",
    "            dataset=infer_data,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        print(f\"running inference for {claim_id} n={len(infer_data)}\")\n",
    "    \n",
    "        # Set model mode to evaluation\n",
    "        model.eval()\n",
    "        \n",
    "        infer_batches = tqdm(infer_dataloader, desc=\"infer batches\")\n",
    "        for batch in infer_batches:\n",
    "            claim_texts, evidence_texts, labels, batch_claim_ids, batch_evidence_ids = batch\n",
    "            texts = list(zip(claim_texts, evidence_texts))\n",
    "            \n",
    "            # Forward\n",
    "            output, logits, seq = model(\n",
    "                texts=texts,\n",
    "                normalize_text=True,\n",
    "                max_length=512,\n",
    "                dropout=None\n",
    "            )\n",
    "            \n",
    "            # Get score as probabilities for label 1\n",
    "            batch_scores = output[:, 1]\n",
    "            \n",
    "            for c_id, e_id, score in zip(\n",
    "                batch_claim_ids, batch_evidence_ids,\n",
    "                batch_scores.cpu().detach().numpy()\n",
    "            ):\n",
    "                claim_predictions[c_id][\"evidences\"].append(EvidenceScore(\n",
    "                    evidence_id=e_id,\n",
    "                    score=score\n",
    "                ))\n",
    "            \n",
    "            continue\n",
    "    \n",
    "        # Save on every claim_id\n",
    "        if save_path:\n",
    "            # Retrieve at most 5 top predicted evidences by score\n",
    "            claim_predictions_output = dict()\n",
    "            for claim_id_, claim_ in claim_predictions.items():\n",
    "                claim_ = claim_.copy()\n",
    "                claim_[\"evidences\"] = [\n",
    "                    # (evidence_score.evidence_id, str(round(evidence_score.score, 3)))\n",
    "                    evidence_score.evidence_id\n",
    "                    for evidence_score in sorted(\n",
    "                        claim_[\"evidences\"],\n",
    "                        key=lambda es: es.score,\n",
    "                        reverse=True\n",
    "                    )\n",
    "                ][:5]\n",
    "                claim_predictions_output[claim_id_] = claim_\n",
    "            \n",
    "            # Make a copy of existing saved results and add the new results\n",
    "            # for this run\n",
    "            save_dict = saved_predictions.copy()\n",
    "            save_dict.update(claim_predictions_output)\n",
    "\n",
    "            with open(save_path, mode=\"w\") as f:\n",
    "                json.dump(obj=save_dict, fp=f)\n",
    "                print(f\"saved to: {save_path}\")\n",
    "    \n",
    "        continue\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inference claims n=154\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 115881.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=873\n",
      "running inference for claim-752 n=873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  51%|█████▏    | 19/37 [00:08<00:06,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 355684.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=946\n",
      "running inference for claim-375 n=946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  52%|█████▎    | 21/40 [00:08<00:07,  2.48it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 455516.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=711\n",
      "running inference for claim-1266 n=711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  50%|█████     | 15/30 [00:05<00:05,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 30/30 [00:11<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 389110.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=378\n",
      "running inference for claim-871 n=378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 16/16 [00:06<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 114667.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=654\n",
      "running inference for claim-2164 n=654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 28/28 [00:11<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 120981.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=890\n",
      "running inference for claim-1607 n=890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 21/38 [00:08<00:06,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:14<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 108086.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=904\n",
      "running inference for claim-761 n=904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 20/38 [00:08<00:07,  2.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:15<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 126750.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=829\n",
      "running inference for claim-1718 n=829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 19/35 [00:07<00:06,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 105113.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=995\n",
      "running inference for claim-1273 n=995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  48%|████▊     | 20/42 [00:08<00:09,  2.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches:  52%|█████▏    | 22/42 [00:09<00:08,  2.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 42/42 [00:17<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 174620.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=836\n",
      "running inference for claim-1786 n=836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 35/35 [00:14<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 104840.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=946\n",
      "running inference for claim-2796 n=946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 411678.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=811\n",
      "running inference for claim-2580 n=811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 109293.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=934\n",
      "running inference for claim-1219 n=934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  56%|█████▋    | 22/39 [00:09<00:06,  2.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 336243.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=930\n",
      "running inference for claim-75 n=930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 21/39 [00:08<00:07,  2.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 103281.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=933\n",
      "running inference for claim-2813 n=933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  56%|█████▋    | 22/39 [00:08<00:06,  2.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 109459.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=944\n",
      "running inference for claim-2335 n=944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 22/40 [00:08<00:07,  2.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 414053.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=862\n",
      "running inference for claim-161 n=862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  56%|█████▌    | 20/36 [00:08<00:06,  2.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 107242.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=980\n",
      "running inference for claim-2243 n=980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 232012.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=739\n",
      "running inference for claim-1256 n=739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▍    | 17/31 [00:06<00:05,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 31/31 [00:12<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 148249.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=970\n",
      "running inference for claim-506 n=970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  49%|████▉     | 20/41 [00:08<00:08,  2.46it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches:  54%|█████▎    | 22/41 [00:09<00:07,  2.46it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 239496.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=949\n",
      "running inference for claim-369 n=949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 513452.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=662\n",
      "running inference for claim-2184 n=662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 28/28 [00:11<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 55515.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=990\n",
      "running inference for claim-1057 n=990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▍    | 23/42 [00:09<00:07,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 42/42 [00:16<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 176771.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=895\n",
      "running inference for claim-104 n=895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 21/38 [00:08<00:06,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:14<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 54669.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=983\n",
      "running inference for claim-1975 n=983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 207625.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=893\n",
      "running inference for claim-139 n=893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 20/38 [00:08<00:07,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:14<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 54206.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=937\n",
      "running inference for claim-2062 n=937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  52%|█████▎    | 21/40 [00:08<00:07,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 384020.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=871\n",
      "running inference for claim-1160 n=871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  51%|█████▏    | 19/37 [00:07<00:07,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 56869.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=957\n",
      "running inference for claim-2679 n=957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 22/40 [00:08<00:07,  2.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:16<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 104113.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=882\n",
      "running inference for claim-2662 n=882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 56186.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=938\n",
      "running inference for claim-1490 n=938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  52%|█████▎    | 21/40 [00:08<00:07,  2.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 111097.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=793\n",
      "running inference for claim-2768 n=793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 56471.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=902\n",
      "running inference for claim-2168 n=902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 21/38 [00:08<00:06,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:15<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 109906.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=914\n",
      "running inference for claim-785 n=914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 21/39 [00:08<00:07,  2.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 45764.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=900\n",
      "running inference for claim-2426 n=900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 21/38 [00:08<00:06,  2.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:14<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 129651.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=426\n",
      "running inference for claim-1292 n=426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 18/18 [00:07<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 105906.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=954\n",
      "running inference for claim-993 n=954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  52%|█████▎    | 21/40 [00:08<00:07,  2.48it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches:  57%|█████▊    | 23/40 [00:09<00:06,  2.48it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:16<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 109925.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=993\n",
      "running inference for claim-2593 n=993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  52%|█████▏    | 22/42 [00:08<00:07,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 42/42 [00:16<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 108832.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=952\n",
      "running inference for claim-1567 n=952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 22/40 [00:08<00:07,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 111442.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=862\n",
      "running inference for claim-1834 n=862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 19/36 [00:07<00:06,  2.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 110376.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=861\n",
      "running inference for claim-856 n=861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  50%|█████     | 18/36 [00:07<00:07,  2.47it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches:  53%|█████▎    | 19/36 [00:07<00:06,  2.46it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 61769.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=594\n",
      "running inference for claim-540 n=594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 25/25 [00:09<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 107242.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=881\n",
      "running inference for claim-757 n=881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 120597.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=639\n",
      "running inference for claim-1407 n=639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 27/27 [00:10<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 108906.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=756\n",
      "running inference for claim-3070 n=756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  50%|█████     | 16/32 [00:06<00:06,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 32/32 [00:12<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 285806.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=867\n",
      "running inference for claim-1745 n=867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 20/37 [00:08<00:06,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 58243.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=868\n",
      "running inference for claim-1515 n=868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 114160.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=898\n",
      "running inference for claim-1519 n=898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 20/38 [00:08<00:07,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:14<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 111174.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=779\n",
      "running inference for claim-3069 n=779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 33/33 [00:13<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 234369.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=521\n",
      "running inference for claim-677 n=521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 22/22 [00:08<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 144598.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=841\n",
      "running inference for claim-765 n=841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 36/36 [00:13<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 48918.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=839\n",
      "running inference for claim-2275 n=839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 119108.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=738\n",
      "running inference for claim-1113 n=738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 31/31 [00:12<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 350854.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=979\n",
      "running inference for claim-2611 n=979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 108540.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=884\n",
      "running inference for claim-2060 n=884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 20/37 [00:08<00:06,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 109441.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=960\n",
      "running inference for claim-2326 n=960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 22/40 [00:08<00:06,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 100314.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=978\n",
      "running inference for claim-1087 n=978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  59%|█████▊    | 24/41 [00:09<00:06,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 107242.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=948\n",
      "running inference for claim-2867 n=948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 106500.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=817\n",
      "running inference for claim-2300 n=817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 115881.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=927\n",
      "running inference for claim-2250 n=927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 104772.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=958\n",
      "running inference for claim-2429 n=958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 40/40 [00:16<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 107923.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=854\n",
      "running inference for claim-3051 n=854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 19/36 [00:07<00:06,  2.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 108068.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=1000\n",
      "running inference for claim-1549 n=1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 42/42 [00:16<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 105611.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=949\n",
      "running inference for claim-261 n=949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 22/40 [00:08<00:07,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 108521.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=848\n",
      "running inference for claim-2230 n=848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 19/36 [00:07<00:06,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 116571.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=819\n",
      "running inference for claim-2579 n=819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 19/35 [00:07<00:06,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 58934.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=897\n",
      "running inference for claim-1416 n=897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  50%|█████     | 19/38 [00:07<00:07,  2.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches:  55%|█████▌    | 21/38 [00:08<00:06,  2.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:15<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 111848.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=804\n",
      "running inference for claim-2497 n=804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 143124.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=963\n",
      "running inference for claim-811 n=963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 106132.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=901\n",
      "running inference for claim-1896 n=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 38/38 [00:14<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 101977.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=939\n",
      "running inference for claim-2819 n=939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 106324.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=961\n",
      "running inference for claim-2643 n=961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  51%|█████     | 21/41 [00:08<00:07,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:15<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 116655.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=769\n",
      "running inference for claim-1775 n=769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 33/33 [00:12<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 193216.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=832\n",
      "running inference for claim-316 n=832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 19/35 [00:07<00:06,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 123597.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=628\n",
      "running inference for claim-896 n=628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 27/27 [00:10<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 59829.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=640\n",
      "running inference for claim-331 n=640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 27/27 [00:10<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 99879.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=978\n",
      "running inference for claim-2574 n=978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:15<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 47299.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=1001\n",
      "running inference for claim-342 n=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▍    | 23/42 [00:09<00:07,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 42/42 [00:16<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 104976.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=964\n",
      "running inference for claim-2034 n=964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:15<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 47459.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=797\n",
      "running inference for claim-578 n=797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 184286.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=624\n",
      "running inference for claim-976 n=624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 26/26 [00:10<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 110470.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=671\n",
      "running inference for claim-1097 n=671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 15/28 [00:06<00:05,  2.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 28/28 [00:11<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 48228.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=772\n",
      "running inference for claim-609 n=772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 33/33 [00:12<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 299732.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=510\n",
      "running inference for claim-173 n=510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 22/22 [00:08<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 143666.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=854\n",
      "running inference for claim-1222 n=854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 119770.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=848\n",
      "running inference for claim-2441 n=848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 117099.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=806\n",
      "running inference for claim-756 n=806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  50%|█████     | 17/34 [00:06<00:06,  2.52it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 49424.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=965\n",
      "running inference for claim-2577 n=965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 105715.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=805\n",
      "running inference for claim-2890 n=805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  56%|█████▌    | 19/34 [00:07<00:05,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 154637.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=854\n",
      "running inference for claim-2478 n=854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 36/36 [00:13<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 110188.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=910\n",
      "running inference for claim-2399 n=910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 21/38 [00:08<00:06,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:15<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 103298.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=973\n",
      "running inference for claim-3091 n=973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 108212.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=761\n",
      "running inference for claim-141 n=761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 32/32 [00:12<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 272541.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=818\n",
      "running inference for claim-1933 n=818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 125421.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=802\n",
      "running inference for claim-1689 n=802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 18/34 [00:07<00:06,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 141525.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=923\n",
      "running inference for claim-443 n=923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 21/39 [00:08<00:07,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 47296.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=924\n",
      "running inference for claim-2037 n=924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  51%|█████▏    | 20/39 [00:08<00:07,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 103480.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=826\n",
      "running inference for claim-1734 n=826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 48690.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=911\n",
      "running inference for claim-2093 n=911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 20/38 [00:08<00:07,  2.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:15<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 113061.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=624\n",
      "running inference for claim-1400 n=624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  58%|█████▊    | 15/26 [00:05<00:04,  2.60it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 26/26 [00:10<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 113399.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=699\n",
      "running inference for claim-1638 n=699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 30/30 [00:11<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 57852.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=878\n",
      "running inference for claim-3075 n=878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 20/37 [00:07<00:06,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 104619.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=972\n",
      "running inference for claim-38 n=972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:15<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 56749.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=822\n",
      "running inference for claim-1643 n=822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 120149.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=860\n",
      "running inference for claim-1259 n=860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 19/36 [00:07<00:06,  2.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 46539.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=935\n",
      "running inference for claim-1605 n=935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 21/39 [00:08<00:06,  2.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 102316.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=998\n",
      "running inference for claim-1711 n=998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▍    | 23/42 [00:09<00:07,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 42/42 [00:16<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 48034.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=800\n",
      "running inference for claim-2236 n=800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  56%|█████▌    | 19/34 [00:07<00:05,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 147673.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=881\n",
      "running inference for claim-1040 n=881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  46%|████▌     | 17/37 [00:07<00:08,  2.48it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 37/37 [00:15<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 46455.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=891\n",
      "running inference for claim-392 n=891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 20/38 [00:07<00:06,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:14<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 99879.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=962\n",
      "running inference for claim-368 n=962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  56%|█████▌    | 23/41 [00:09<00:07,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:15<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 109701.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=676\n",
      "running inference for claim-559 n=676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 29/29 [00:11<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 101512.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=918\n",
      "running inference for claim-2583 n=918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  51%|█████▏    | 20/39 [00:08<00:07,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 101800.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=948\n",
      "running inference for claim-2609 n=948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 22/40 [00:08<00:07,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 104332.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=833\n",
      "running inference for claim-492 n=833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 110869.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=992\n",
      "running inference for claim-1420 n=992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 42/42 [00:16<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 102804.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=860\n",
      "running inference for claim-1089 n=860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  50%|█████     | 18/36 [00:07<00:07,  2.46it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 107153.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=885\n",
      "running inference for claim-1467 n=885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 20/37 [00:07<00:06,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 101656.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=995\n",
      "running inference for claim-444 n=995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▍    | 23/42 [00:09<00:07,  2.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 42/42 [00:16<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 105975.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=884\n",
      "running inference for claim-803 n=884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 20/37 [00:08<00:06,  2.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 37/37 [00:14<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 105698.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=952\n",
      "running inference for claim-1668 n=952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  52%|█████▎    | 21/40 [00:08<00:07,  2.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 106219.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=831\n",
      "running inference for claim-742 n=831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 19/35 [00:07<00:06,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 112256.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=855\n",
      "running inference for claim-846 n=855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  50%|█████     | 18/36 [00:07<00:07,  2.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches:  56%|█████▌    | 20/36 [00:08<00:06,  2.48it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 120015.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=676\n",
      "running inference for claim-2119 n=676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 29/29 [00:10<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 48002.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=922\n",
      "running inference for claim-1167 n=922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 117654.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=316\n",
      "running inference for claim-623 n=316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 14/14 [00:05<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 104265.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=925\n",
      "running inference for claim-2882 n=925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 57914.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=840\n",
      "running inference for claim-1698 n=840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 19/35 [00:07<00:06,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 107528.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=868\n",
      "running inference for claim-181 n=868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 37/37 [00:13<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 47501.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=758\n",
      "running inference for claim-281 n=758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 32/32 [00:12<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 287460.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=759\n",
      "running inference for claim-2809 n=759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 17/32 [00:06<00:05,  2.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 32/32 [00:12<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 266469.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=983\n",
      "running inference for claim-1928 n=983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 41/41 [00:15<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 99617.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=928\n",
      "running inference for claim-2787 n=928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 21/39 [00:08<00:07,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 103779.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=923\n",
      "running inference for claim-478 n=923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 39/39 [00:14<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 116676.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=356\n",
      "running inference for claim-988 n=356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 15/15 [00:05<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 100720.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=894\n",
      "running inference for claim-266 n=894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 21/38 [00:08<00:06,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 38/38 [00:14<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 44756.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=923\n",
      "running inference for claim-2282 n=923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  51%|█████▏    | 20/39 [00:07<00:07,  2.61it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:14<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 100720.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=982\n",
      "running inference for claim-2895 n=982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▎    | 22/41 [00:08<00:07,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 41/41 [00:16<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 45832.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=753\n",
      "running inference for claim-349 n=753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 17/32 [00:06<00:05,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 32/32 [00:12<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 308463.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=926\n",
      "running inference for claim-2101 n=926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 39/39 [00:14<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 110735.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=708\n",
      "running inference for claim-897 n=708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 16/30 [00:06<00:05,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 30/30 [00:11<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 102316.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=831\n",
      "running inference for claim-3063 n=831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  54%|█████▍    | 19/35 [00:07<00:06,  2.58it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 35/35 [00:13<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 114769.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=759\n",
      "running inference for claim-386 n=759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  47%|████▋     | 15/32 [00:06<00:06,  2.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 32/32 [00:12<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 57785.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=694\n",
      "running inference for claim-2691 n=694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 29/29 [00:11<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 101528.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=945\n",
      "running inference for claim-530 n=945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 46150.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=987\n",
      "running inference for claim-2979 n=987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches: 100%|██████████| 42/42 [00:16<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 98015.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=926\n",
      "running inference for claim-665 n=926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  49%|████▊     | 19/39 [00:07<00:08,  2.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches:  54%|█████▍    | 21/39 [00:08<00:07,  2.48it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 39/39 [00:15<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 108686.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=782\n",
      "running inference for claim-199 n=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▍    | 18/33 [00:07<00:05,  2.56it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 33/33 [00:12<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 199235.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=842\n",
      "running inference for claim-490 n=842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 19/36 [00:07<00:06,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:13<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 103629.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=952\n",
      "running inference for claim-2400 n=952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  52%|█████▎    | 21/40 [00:08<00:07,  2.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 104400.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=805\n",
      "running inference for claim-204 n=805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 18/34 [00:07<00:06,  2.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 137635.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=949\n",
      "running inference for claim-1426 n=949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  55%|█████▌    | 22/40 [00:08<00:07,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 103696.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=816\n",
      "running inference for claim-698 n=816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  50%|█████     | 17/34 [00:07<00:06,  2.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches:  56%|█████▌    | 19/34 [00:07<00:06,  2.50it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 34/34 [00:13<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n",
      "Torch device is 'mps'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claims: 100%|██████████| 154/154 [00:00<00:00, 124985.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated dataset n=855\n",
      "running inference for claim-1021 n=855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "infer batches:  53%|█████▎    | 19/36 [00:07<00:06,  2.55it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "infer batches: 100%|██████████| 36/36 [00:14<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: /Users/johnsonzhou/git/comp90042-project/result/pipeline/retrieval_classif/model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_run(\n",
    "    claims_path=DATA_PATH.with_name(\"dev-claims.json\"),\n",
    "    evidence_path=DATA_PATH.with_name(\"evidence.json\"),\n",
    "    claims_shortlist_path=SHORTLIST_PATH.with_name(\"dev_retrieved_evidences_max_1000_rel.json\"),\n",
    "    # evidence_shortlist=NER_PATH.with_name(\"shortlist_dev_claim_evidence_retrieved.json\"),\n",
    "    save_path=OUTPUT_PATH.with_name(\"model_05_bert_base_2023_05_08_17_06_dev_shortlist_max_1000_with_rel.json\"),\n",
    "    batch_size=24\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp90042_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
