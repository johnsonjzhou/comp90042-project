"""
Data I/O functions
"""
import pathlib
import pandas as pd
from pandas import DataFrame
import re
import json
from typing import List, Dict, Union
from collections import defaultdict

# Colors for printing to the terminal
# Ref: https://stackoverflow.com/a/287944
class COLORS:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    END = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


"""
Help with serialising sets to JSON
Ref: https://stackoverflow.com/a/8230505
"""
class SetEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, set):
            return list(obj)
        return json.JSONEncoder.default(self, obj)


def load_from_json(names:list) -> List[dict]:
    DATA_PATH = pathlib.Path("data", "*")
    datasets = list()
    for name in names:
        with open(DATA_PATH.with_name(name).with_suffix(".json")) as json_file:
            dataset = json.load(json_file)
            datasets.append(dataset)
            print(f"Loaded {name}")
    return datasets

def load_as_dataframe(names:list, full_evidence:bool=False) -> List[DataFrame]:
    """
    Gets the json datasets as dataframes

    Args:
        names (list): List of filenames names minus the json suffix.
        full_evidence (bool): Whether to return the full evidence dataset
            as the final element.

    Returns:
        DefaultDict[str, pd.DataFrame]: Keys as names, values as the dataset
    """
    # Load datasets from json as dict
    datasets = load_from_json(names=names)

    # Process the names so that they are snake case
    var_names = [re.sub(pattern=r"-", repl="_", string=name) for name in names]

    # Name the datasets
    named_datasets = zip(var_names, datasets)

    # Find the evidence if available
    for var_name, dataset in named_datasets:
        if var_name == "evidence":
            df = DataFrame.from_dict(dataset, orient="index")
            df.columns = ["evidence_text"]
            locals()[var_name] = df

    # Parse each dataset into a DataFrame then join evidence if available
    datasets_df = list()
    for var_name, dataset in zip(var_names, datasets):
        if var_name == "evidence":
            continue
        df = DataFrame.from_dict(dataset, orient="index")
        if "evidence" in locals().keys():
            df = pd.merge(
                left=df.explode(column="evidences"),
                right=locals()["evidence"],
                how="left",
                left_on="evidences",
                right_index=True
            )
        df = df.reset_index(names=["claim"]) \
            .set_index(keys=["claim", "claim_text", "claim_label", "evidences"])
        datasets_df.append(df)

    # If required, attach the full evidence as the final element
    if full_evidence and "evidence" in locals():
        datasets_df.append(locals()["evidence"])

    return datasets_df

def slice_by_claim(
    dataset_df:DataFrame, start:int=None, end:int=1, labels:List[str]=None
) -> DataFrame:
    """
    Slice the dataset by a given start and end sequence index,
    optionally accept claim labels to filter by.

    Args:
        dataset_df (DataFrame): Dataset generated by `load_as_dataframe`.
        start (int, optional): Starting index. Defaults to None.
        end (int, optional): Ending index. Defaults to 1.
        labels (List[str], optional): Claim class labels. Defaults to None.

    Returns:
        DataFrame: A subset of `dataset_df`.
    """
    # Query by label if required
    if labels is not None:
        dataset_df = dataset_df.query(f"claim_label == {labels}")

    # Create the slice
    claim_slice = (
        dataset_df
        .loc[dataset_df.index.get_level_values("claim").unique()[start:end]]
    )
    return claim_slice

def get_claim_texts(dataset_df:DataFrame) -> List[str]:
    """
    Retrieve unique claim texts from a dataset.

    Args:
        dataset_df (DataFrame): Dataset generated by `load_as_dataframe`.

    Returns:
        List[str]: A list of claim texts.
    """
    return dataset_df.index.get_level_values("claim_text").unique().to_list()

def get_evidence_texts(dataset_df:DataFrame) -> List[str]:
    """
    Retrieve all the evidence texts from a dataset.

    Args:
        dataset_df (DataFrame): Dataset generated by `load_as_dataframe`.

    Returns:
        List[str]: A list of claim texts.
    """
    return dataset_df["evidence_text"].to_list()

def get_paired_texts(dataset_df:DataFrame) -> DataFrame:
    """
    Retrieve claim and evidence text from a dataset as ordered pair.

    Args:
        dataset_df (DataFrame): Dataset generated by `load_as_dataframe`.

    Returns:
        DataFrame: `index` is claim-evidence pairs, columns are
            `claim_text`, `evidence_text`.
    """
    claim_evidence_pairs = (
        dataset_df
        .reset_index()[["claim_text", "evidence_text"]]
    )
    return claim_evidence_pairs

def create_claim_output(
    claim_id:str,
    claim_text:str,
    claim_label:str="NOT_ENOUGH_INFO",
    evidences:List[str] = list()
) -> Dict[str, Dict[str, Union[str, List[str]]]]:
    """
    Generates a consistent claim object to write to JSON.

    Args:
        claim_id (str): The claim ID.
        claim_text (str): The claim text.
        claim_label (str, optional): Claim label. Defaults to "NOT_ENOUGH_INFO".
        evidences (List[str], optional): List of evidences. Defaults to list().

    Returns:
        Dict[str, Dict[str, Union[str, List[str]]]]: The claim dict.
    """
    claim_dict = {
        claim_id: {
            "claim_text": claim_text,
            "claim_label": claim_label,
            "evidences": evidences
        }
    }
    return claim_dict